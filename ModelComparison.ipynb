{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1866f89-a655-4a39-99d5-bec2bdd132f9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37177014-7e79-423d-b5b8-be4bdce512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import os.path as osp\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "# For using the mgbdt library, you have to include the library directory into your python path.\n",
    "# If you are in this repository's root directory, you can do it by using the following lines\n",
    "import sys,os\n",
    "# sys.path.insert(0, os.path.join('mGBDT','lib'))\n",
    "sys.path.insert(0, os.path.join('lib'))\n",
    "\n",
    "from mgbdt import MGBDT\n",
    "from mgbdt import MultiXGBModel, LinearModel\n",
    "from mgbdt.utils.plot_utils import plot2d, plot3d\n",
    "from mgbdt.utils.exp_utils import set_seed\n",
    "from mgbdt.utils.log_utils import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07f372-c724-40c8-b169-31caa63ea952",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34230c15-4335-4048-8fbf-4e8d21af75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boston = datasets.load_boston()\n",
    "# X, y = boston.data, boston.target\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "x_mean = X.mean(axis=0)\n",
    "x_std = X.std(axis=0)\n",
    "X = (X - x_mean) / x_std\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df76023-b8a0-4292-b47a-f616645006dc",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce9f009-e146-4097-b781-b4002ccb642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "num_boost_round = 5\n",
    "learning_rate = 0.03\n",
    "max_depth = 5\n",
    "loss = \"L1Loss\"\n",
    "\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda6fc8-6a3b-4d2e-955c-3345bf8879fe",
   "metadata": {},
   "source": [
    "## mGBDT Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9753dd5-2cf6-4a79-a761-e0a80c00b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\xgboost\\core.py:613: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\n",
      "[ 2025-02-07 18:23:22,856][mgbdt.log] \u001b[32m[epoch=0/200][train] loss=602.797764\u001b[0m\n",
      "[ 2025-02-07 18:23:23,185][mgbdt.log] \u001b[32m[epoch=1/200][train] loss=601.589081\u001b[0m\n",
      "[ 2025-02-07 18:23:23,491][mgbdt.log] \u001b[32m[epoch=2/200][train] loss=600.068914\u001b[0m\n",
      "[ 2025-02-07 18:23:23,798][mgbdt.log] \u001b[32m[epoch=3/200][train] loss=598.612313\u001b[0m\n",
      "[ 2025-02-07 18:23:24,115][mgbdt.log] \u001b[32m[epoch=4/200][train] loss=597.232585\u001b[0m\n",
      "[ 2025-02-07 18:23:24,432][mgbdt.log] \u001b[32m[epoch=5/200][train] loss=595.811719\u001b[0m\n",
      "[ 2025-02-07 18:23:24,751][mgbdt.log] \u001b[32m[epoch=6/200][train] loss=594.468897\u001b[0m\n",
      "[ 2025-02-07 18:23:25,052][mgbdt.log] \u001b[32m[epoch=7/200][train] loss=593.027381\u001b[0m\n",
      "[ 2025-02-07 18:23:25,360][mgbdt.log] \u001b[32m[epoch=8/200][train] loss=591.632478\u001b[0m\n",
      "[ 2025-02-07 18:23:25,668][mgbdt.log] \u001b[32m[epoch=9/200][train] loss=590.312126\u001b[0m\n",
      "[ 2025-02-07 18:23:26,017][mgbdt.log] \u001b[32m[epoch=10/200][train] loss=588.995318\u001b[0m\n",
      "[ 2025-02-07 18:23:26,361][mgbdt.log] \u001b[32m[epoch=11/200][train] loss=587.706287\u001b[0m\n",
      "[ 2025-02-07 18:23:26,850][mgbdt.log] \u001b[32m[epoch=12/200][train] loss=586.370398\u001b[0m\n",
      "[ 2025-02-07 18:23:27,186][mgbdt.log] \u001b[32m[epoch=13/200][train] loss=585.015023\u001b[0m\n",
      "[ 2025-02-07 18:23:27,546][mgbdt.log] \u001b[32m[epoch=14/200][train] loss=583.646347\u001b[0m\n",
      "[ 2025-02-07 18:23:27,897][mgbdt.log] \u001b[32m[epoch=15/200][train] loss=582.284524\u001b[0m\n",
      "[ 2025-02-07 18:23:28,253][mgbdt.log] \u001b[32m[epoch=16/200][train] loss=580.879557\u001b[0m\n",
      "[ 2025-02-07 18:23:28,596][mgbdt.log] \u001b[32m[epoch=17/200][train] loss=579.453250\u001b[0m\n",
      "[ 2025-02-07 18:23:28,925][mgbdt.log] \u001b[32m[epoch=18/200][train] loss=578.033564\u001b[0m\n",
      "[ 2025-02-07 18:23:29,291][mgbdt.log] \u001b[32m[epoch=19/200][train] loss=576.535250\u001b[0m\n",
      "[ 2025-02-07 18:23:29,669][mgbdt.log] \u001b[32m[epoch=20/200][train] loss=575.036873\u001b[0m\n",
      "[ 2025-02-07 18:23:30,076][mgbdt.log] \u001b[32m[epoch=21/200][train] loss=573.452480\u001b[0m\n",
      "[ 2025-02-07 18:23:30,494][mgbdt.log] \u001b[32m[epoch=22/200][train] loss=571.785762\u001b[0m\n",
      "[ 2025-02-07 18:23:30,951][mgbdt.log] \u001b[32m[epoch=23/200][train] loss=570.040272\u001b[0m\n",
      "[ 2025-02-07 18:23:31,437][mgbdt.log] \u001b[32m[epoch=24/200][train] loss=568.235959\u001b[0m\n",
      "[ 2025-02-07 18:23:31,880][mgbdt.log] \u001b[32m[epoch=25/200][train] loss=566.287352\u001b[0m\n",
      "[ 2025-02-07 18:23:32,374][mgbdt.log] \u001b[32m[epoch=26/200][train] loss=564.247363\u001b[0m\n",
      "[ 2025-02-07 18:23:32,833][mgbdt.log] \u001b[32m[epoch=27/200][train] loss=562.076396\u001b[0m\n",
      "[ 2025-02-07 18:23:33,315][mgbdt.log] \u001b[32m[epoch=28/200][train] loss=559.792176\u001b[0m\n",
      "[ 2025-02-07 18:23:33,715][mgbdt.log] \u001b[32m[epoch=29/200][train] loss=557.411928\u001b[0m\n",
      "[ 2025-02-07 18:23:34,126][mgbdt.log] \u001b[32m[epoch=30/200][train] loss=554.894856\u001b[0m\n",
      "[ 2025-02-07 18:23:34,522][mgbdt.log] \u001b[32m[epoch=31/200][train] loss=552.227669\u001b[0m\n",
      "[ 2025-02-07 18:23:34,979][mgbdt.log] \u001b[32m[epoch=32/200][train] loss=549.385411\u001b[0m\n",
      "[ 2025-02-07 18:23:35,388][mgbdt.log] \u001b[32m[epoch=33/200][train] loss=546.314237\u001b[0m\n",
      "[ 2025-02-07 18:23:35,823][mgbdt.log] \u001b[32m[epoch=34/200][train] loss=542.990758\u001b[0m\n",
      "[ 2025-02-07 18:23:36,273][mgbdt.log] \u001b[32m[epoch=35/200][train] loss=539.568507\u001b[0m\n",
      "[ 2025-02-07 18:23:36,727][mgbdt.log] \u001b[32m[epoch=36/200][train] loss=535.956397\u001b[0m\n",
      "[ 2025-02-07 18:23:37,257][mgbdt.log] \u001b[32m[epoch=37/200][train] loss=532.229942\u001b[0m\n",
      "[ 2025-02-07 18:23:37,726][mgbdt.log] \u001b[32m[epoch=38/200][train] loss=528.406323\u001b[0m\n",
      "[ 2025-02-07 18:23:38,236][mgbdt.log] \u001b[32m[epoch=39/200][train] loss=524.398430\u001b[0m\n",
      "[ 2025-02-07 18:23:38,708][mgbdt.log] \u001b[32m[epoch=40/200][train] loss=520.228473\u001b[0m\n",
      "[ 2025-02-07 18:23:39,147][mgbdt.log] \u001b[32m[epoch=41/200][train] loss=516.143660\u001b[0m\n",
      "[ 2025-02-07 18:23:39,587][mgbdt.log] \u001b[32m[epoch=42/200][train] loss=511.473417\u001b[0m\n",
      "[ 2025-02-07 18:23:40,072][mgbdt.log] \u001b[32m[epoch=43/200][train] loss=506.414483\u001b[0m\n",
      "[ 2025-02-07 18:23:40,607][mgbdt.log] \u001b[32m[epoch=44/200][train] loss=501.049677\u001b[0m\n",
      "[ 2025-02-07 18:23:41,116][mgbdt.log] \u001b[32m[epoch=45/200][train] loss=495.337757\u001b[0m\n",
      "[ 2025-02-07 18:23:41,627][mgbdt.log] \u001b[32m[epoch=46/200][train] loss=489.232970\u001b[0m\n",
      "[ 2025-02-07 18:23:42,153][mgbdt.log] \u001b[32m[epoch=47/200][train] loss=482.636166\u001b[0m\n",
      "[ 2025-02-07 18:23:42,629][mgbdt.log] \u001b[32m[epoch=48/200][train] loss=475.601003\u001b[0m\n",
      "[ 2025-02-07 18:23:43,076][mgbdt.log] \u001b[32m[epoch=49/200][train] loss=468.027769\u001b[0m\n",
      "[ 2025-02-07 18:23:43,526][mgbdt.log] \u001b[32m[epoch=50/200][train] loss=460.092564\u001b[0m\n",
      "[ 2025-02-07 18:23:44,011][mgbdt.log] \u001b[32m[epoch=51/200][train] loss=451.669882\u001b[0m\n",
      "[ 2025-02-07 18:23:44,521][mgbdt.log] \u001b[32m[epoch=52/200][train] loss=442.036481\u001b[0m\n",
      "[ 2025-02-07 18:23:45,032][mgbdt.log] \u001b[32m[epoch=53/200][train] loss=431.734624\u001b[0m\n",
      "[ 2025-02-07 18:23:45,528][mgbdt.log] \u001b[32m[epoch=54/200][train] loss=421.222530\u001b[0m\n",
      "[ 2025-02-07 18:23:46,075][mgbdt.log] \u001b[32m[epoch=55/200][train] loss=410.582735\u001b[0m\n",
      "[ 2025-02-07 18:23:46,582][mgbdt.log] \u001b[32m[epoch=56/200][train] loss=399.112308\u001b[0m\n",
      "[ 2025-02-07 18:23:47,113][mgbdt.log] \u001b[32m[epoch=57/200][train] loss=387.100302\u001b[0m\n",
      "[ 2025-02-07 18:23:47,631][mgbdt.log] \u001b[32m[epoch=58/200][train] loss=374.292722\u001b[0m\n",
      "[ 2025-02-07 18:23:48,105][mgbdt.log] \u001b[32m[epoch=59/200][train] loss=360.515419\u001b[0m\n",
      "[ 2025-02-07 18:23:48,589][mgbdt.log] \u001b[32m[epoch=60/200][train] loss=345.483988\u001b[0m\n",
      "[ 2025-02-07 18:23:49,066][mgbdt.log] \u001b[32m[epoch=61/200][train] loss=329.626825\u001b[0m\n",
      "[ 2025-02-07 18:23:49,589][mgbdt.log] \u001b[32m[epoch=62/200][train] loss=313.638194\u001b[0m\n",
      "[ 2025-02-07 18:23:50,130][mgbdt.log] \u001b[32m[epoch=63/200][train] loss=297.231783\u001b[0m\n",
      "[ 2025-02-07 18:23:50,667][mgbdt.log] \u001b[32m[epoch=64/200][train] loss=280.739999\u001b[0m\n",
      "[ 2025-02-07 18:23:51,183][mgbdt.log] \u001b[32m[epoch=65/200][train] loss=263.618805\u001b[0m\n",
      "[ 2025-02-07 18:23:51,717][mgbdt.log] \u001b[32m[epoch=66/200][train] loss=245.949817\u001b[0m\n",
      "[ 2025-02-07 18:23:52,247][mgbdt.log] \u001b[32m[epoch=67/200][train] loss=228.303396\u001b[0m\n",
      "[ 2025-02-07 18:23:52,891][mgbdt.log] \u001b[32m[epoch=68/200][train] loss=211.033041\u001b[0m\n",
      "[ 2025-02-07 18:23:53,471][mgbdt.log] \u001b[32m[epoch=69/200][train] loss=194.079690\u001b[0m\n",
      "[ 2025-02-07 18:23:54,015][mgbdt.log] \u001b[32m[epoch=70/200][train] loss=177.582848\u001b[0m\n",
      "[ 2025-02-07 18:23:54,564][mgbdt.log] \u001b[32m[epoch=71/200][train] loss=161.708295\u001b[0m\n",
      "[ 2025-02-07 18:23:55,126][mgbdt.log] \u001b[32m[epoch=72/200][train] loss=146.956180\u001b[0m\n",
      "[ 2025-02-07 18:23:55,692][mgbdt.log] \u001b[32m[epoch=73/200][train] loss=132.530757\u001b[0m\n",
      "[ 2025-02-07 18:23:56,269][mgbdt.log] \u001b[32m[epoch=74/200][train] loss=120.012042\u001b[0m\n",
      "[ 2025-02-07 18:23:56,814][mgbdt.log] \u001b[32m[epoch=75/200][train] loss=107.863576\u001b[0m\n",
      "[ 2025-02-07 18:23:57,387][mgbdt.log] \u001b[32m[epoch=76/200][train] loss=96.874079\u001b[0m\n",
      "[ 2025-02-07 18:23:57,954][mgbdt.log] \u001b[32m[epoch=77/200][train] loss=88.020195\u001b[0m\n",
      "[ 2025-02-07 18:23:58,598][mgbdt.log] \u001b[32m[epoch=78/200][train] loss=79.002676\u001b[0m\n",
      "[ 2025-02-07 18:23:59,182][mgbdt.log] \u001b[32m[epoch=79/200][train] loss=72.146888\u001b[0m\n",
      "[ 2025-02-07 18:23:59,742][mgbdt.log] \u001b[32m[epoch=80/200][train] loss=65.018268\u001b[0m\n",
      "[ 2025-02-07 18:24:00,343][mgbdt.log] \u001b[32m[epoch=81/200][train] loss=58.758640\u001b[0m\n",
      "[ 2025-02-07 18:24:00,944][mgbdt.log] \u001b[32m[epoch=82/200][train] loss=54.396303\u001b[0m\n",
      "[ 2025-02-07 18:24:01,518][mgbdt.log] \u001b[32m[epoch=83/200][train] loss=49.510786\u001b[0m\n",
      "[ 2025-02-07 18:24:02,111][mgbdt.log] \u001b[32m[epoch=84/200][train] loss=46.856045\u001b[0m\n",
      "[ 2025-02-07 18:24:02,696][mgbdt.log] \u001b[32m[epoch=85/200][train] loss=44.284915\u001b[0m\n",
      "[ 2025-02-07 18:24:03,298][mgbdt.log] \u001b[32m[epoch=86/200][train] loss=42.588930\u001b[0m\n",
      "[ 2025-02-07 18:24:03,924][mgbdt.log] \u001b[32m[epoch=87/200][train] loss=40.012727\u001b[0m\n",
      "[ 2025-02-07 18:24:04,552][mgbdt.log] \u001b[32m[epoch=88/200][train] loss=37.997439\u001b[0m\n",
      "[ 2025-02-07 18:24:05,192][mgbdt.log] \u001b[32m[epoch=89/200][train] loss=36.823258\u001b[0m\n",
      "[ 2025-02-07 18:24:05,847][mgbdt.log] \u001b[32m[epoch=90/200][train] loss=35.171574\u001b[0m\n",
      "[ 2025-02-07 18:24:06,479][mgbdt.log] \u001b[32m[epoch=91/200][train] loss=33.244091\u001b[0m\n",
      "[ 2025-02-07 18:24:07,112][mgbdt.log] \u001b[32m[epoch=92/200][train] loss=31.119581\u001b[0m\n",
      "[ 2025-02-07 18:24:07,763][mgbdt.log] \u001b[32m[epoch=93/200][train] loss=28.879442\u001b[0m\n",
      "[ 2025-02-07 18:24:08,407][mgbdt.log] \u001b[32m[epoch=94/200][train] loss=26.875215\u001b[0m\n",
      "[ 2025-02-07 18:24:09,059][mgbdt.log] \u001b[32m[epoch=95/200][train] loss=24.886366\u001b[0m\n",
      "[ 2025-02-07 18:24:09,722][mgbdt.log] \u001b[32m[epoch=96/200][train] loss=23.833447\u001b[0m\n",
      "[ 2025-02-07 18:24:10,394][mgbdt.log] \u001b[32m[epoch=97/200][train] loss=20.594918\u001b[0m\n",
      "[ 2025-02-07 18:24:11,054][mgbdt.log] \u001b[32m[epoch=98/200][train] loss=19.462230\u001b[0m\n",
      "[ 2025-02-07 18:24:11,714][mgbdt.log] \u001b[32m[epoch=99/200][train] loss=20.134463\u001b[0m\n",
      "[ 2025-02-07 18:24:12,406][mgbdt.log] \u001b[32m[epoch=100/200][train] loss=20.781454\u001b[0m\n",
      "[ 2025-02-07 18:24:13,076][mgbdt.log] \u001b[32m[epoch=101/200][train] loss=21.076071\u001b[0m\n",
      "[ 2025-02-07 18:24:13,758][mgbdt.log] \u001b[32m[epoch=102/200][train] loss=20.891683\u001b[0m\n",
      "[ 2025-02-07 18:24:14,444][mgbdt.log] \u001b[32m[epoch=103/200][train] loss=21.225352\u001b[0m\n",
      "[ 2025-02-07 18:24:15,121][mgbdt.log] \u001b[32m[epoch=104/200][train] loss=20.952404\u001b[0m\n",
      "[ 2025-02-07 18:24:15,814][mgbdt.log] \u001b[32m[epoch=105/200][train] loss=21.225129\u001b[0m\n",
      "[ 2025-02-07 18:24:16,522][mgbdt.log] \u001b[32m[epoch=106/200][train] loss=21.312048\u001b[0m\n",
      "[ 2025-02-07 18:24:17,265][mgbdt.log] \u001b[32m[epoch=107/200][train] loss=21.886063\u001b[0m\n",
      "[ 2025-02-07 18:24:17,979][mgbdt.log] \u001b[32m[epoch=108/200][train] loss=22.845445\u001b[0m\n",
      "[ 2025-02-07 18:24:18,701][mgbdt.log] \u001b[32m[epoch=109/200][train] loss=22.652249\u001b[0m\n",
      "[ 2025-02-07 18:24:19,436][mgbdt.log] \u001b[32m[epoch=110/200][train] loss=23.223451\u001b[0m\n",
      "[ 2025-02-07 18:24:20,163][mgbdt.log] \u001b[32m[epoch=111/200][train] loss=22.598732\u001b[0m\n",
      "[ 2025-02-07 18:24:20,906][mgbdt.log] \u001b[32m[epoch=112/200][train] loss=22.343230\u001b[0m\n",
      "[ 2025-02-07 18:24:21,640][mgbdt.log] \u001b[32m[epoch=113/200][train] loss=22.023612\u001b[0m\n",
      "[ 2025-02-07 18:24:22,367][mgbdt.log] \u001b[32m[epoch=114/200][train] loss=21.610712\u001b[0m\n",
      "[ 2025-02-07 18:24:23,118][mgbdt.log] \u001b[32m[epoch=115/200][train] loss=22.230017\u001b[0m\n",
      "[ 2025-02-07 18:24:23,889][mgbdt.log] \u001b[32m[epoch=116/200][train] loss=22.309788\u001b[0m\n",
      "[ 2025-02-07 18:24:24,713][mgbdt.log] \u001b[32m[epoch=117/200][train] loss=22.236881\u001b[0m\n",
      "[ 2025-02-07 18:24:25,508][mgbdt.log] \u001b[32m[epoch=118/200][train] loss=21.416308\u001b[0m\n",
      "[ 2025-02-07 18:24:26,242][mgbdt.log] \u001b[32m[epoch=119/200][train] loss=20.124206\u001b[0m\n",
      "[ 2025-02-07 18:24:27,000][mgbdt.log] \u001b[32m[epoch=120/200][train] loss=19.639872\u001b[0m\n",
      "[ 2025-02-07 18:24:27,789][mgbdt.log] \u001b[32m[epoch=121/200][train] loss=19.178399\u001b[0m\n",
      "[ 2025-02-07 18:24:28,570][mgbdt.log] \u001b[32m[epoch=122/200][train] loss=18.526527\u001b[0m\n",
      "[ 2025-02-07 18:24:29,373][mgbdt.log] \u001b[32m[epoch=123/200][train] loss=17.978554\u001b[0m\n",
      "[ 2025-02-07 18:24:30,175][mgbdt.log] \u001b[32m[epoch=124/200][train] loss=17.212465\u001b[0m\n",
      "[ 2025-02-07 18:24:30,968][mgbdt.log] \u001b[32m[epoch=125/200][train] loss=17.137209\u001b[0m\n",
      "[ 2025-02-07 18:24:31,776][mgbdt.log] \u001b[32m[epoch=126/200][train] loss=16.491317\u001b[0m\n",
      "[ 2025-02-07 18:24:32,621][mgbdt.log] \u001b[32m[epoch=127/200][train] loss=15.886234\u001b[0m\n",
      "[ 2025-02-07 18:24:33,454][mgbdt.log] \u001b[32m[epoch=128/200][train] loss=15.135399\u001b[0m\n",
      "[ 2025-02-07 18:24:34,316][mgbdt.log] \u001b[32m[epoch=129/200][train] loss=15.325073\u001b[0m\n",
      "[ 2025-02-07 18:24:35,134][mgbdt.log] \u001b[32m[epoch=130/200][train] loss=14.902268\u001b[0m\n",
      "[ 2025-02-07 18:24:35,988][mgbdt.log] \u001b[32m[epoch=131/200][train] loss=14.770404\u001b[0m\n",
      "[ 2025-02-07 18:24:36,836][mgbdt.log] \u001b[32m[epoch=132/200][train] loss=15.146393\u001b[0m\n",
      "[ 2025-02-07 18:24:37,705][mgbdt.log] \u001b[32m[epoch=133/200][train] loss=14.726666\u001b[0m\n",
      "[ 2025-02-07 18:24:38,590][mgbdt.log] \u001b[32m[epoch=134/200][train] loss=14.438652\u001b[0m\n",
      "[ 2025-02-07 18:24:39,491][mgbdt.log] \u001b[32m[epoch=135/200][train] loss=14.634470\u001b[0m\n",
      "[ 2025-02-07 18:24:40,393][mgbdt.log] \u001b[32m[epoch=136/200][train] loss=14.329403\u001b[0m\n",
      "[ 2025-02-07 18:24:41,244][mgbdt.log] \u001b[32m[epoch=137/200][train] loss=14.979703\u001b[0m\n",
      "[ 2025-02-07 18:24:42,113][mgbdt.log] \u001b[32m[epoch=138/200][train] loss=15.241139\u001b[0m\n",
      "[ 2025-02-07 18:24:42,991][mgbdt.log] \u001b[32m[epoch=139/200][train] loss=15.567761\u001b[0m\n",
      "[ 2025-02-07 18:24:43,899][mgbdt.log] \u001b[32m[epoch=140/200][train] loss=15.713534\u001b[0m\n",
      "[ 2025-02-07 18:24:44,811][mgbdt.log] \u001b[32m[epoch=141/200][train] loss=14.513937\u001b[0m\n",
      "[ 2025-02-07 18:24:45,728][mgbdt.log] \u001b[32m[epoch=142/200][train] loss=15.020021\u001b[0m\n",
      "[ 2025-02-07 18:24:46,638][mgbdt.log] \u001b[32m[epoch=143/200][train] loss=15.286863\u001b[0m\n",
      "[ 2025-02-07 18:24:47,549][mgbdt.log] \u001b[32m[epoch=144/200][train] loss=15.358965\u001b[0m\n",
      "[ 2025-02-07 18:24:48,468][mgbdt.log] \u001b[32m[epoch=145/200][train] loss=15.321625\u001b[0m\n",
      "[ 2025-02-07 18:24:49,359][mgbdt.log] \u001b[32m[epoch=146/200][train] loss=15.223657\u001b[0m\n",
      "[ 2025-02-07 18:24:50,312][mgbdt.log] \u001b[32m[epoch=147/200][train] loss=14.523438\u001b[0m\n",
      "[ 2025-02-07 18:24:51,264][mgbdt.log] \u001b[32m[epoch=148/200][train] loss=14.637167\u001b[0m\n",
      "[ 2025-02-07 18:24:52,198][mgbdt.log] \u001b[32m[epoch=149/200][train] loss=14.305145\u001b[0m\n",
      "[ 2025-02-07 18:24:53,168][mgbdt.log] \u001b[32m[epoch=150/200][train] loss=14.424031\u001b[0m\n",
      "[ 2025-02-07 18:24:54,138][mgbdt.log] \u001b[32m[epoch=151/200][train] loss=13.969140\u001b[0m\n",
      "[ 2025-02-07 18:24:55,067][mgbdt.log] \u001b[32m[epoch=152/200][train] loss=13.290175\u001b[0m\n",
      "[ 2025-02-07 18:24:56,014][mgbdt.log] \u001b[32m[epoch=153/200][train] loss=12.668922\u001b[0m\n",
      "[ 2025-02-07 18:24:56,957][mgbdt.log] \u001b[32m[epoch=154/200][train] loss=12.207088\u001b[0m\n",
      "[ 2025-02-07 18:24:57,907][mgbdt.log] \u001b[32m[epoch=155/200][train] loss=11.593721\u001b[0m\n",
      "[ 2025-02-07 18:24:58,853][mgbdt.log] \u001b[32m[epoch=156/200][train] loss=10.927690\u001b[0m\n",
      "[ 2025-02-07 18:24:59,796][mgbdt.log] \u001b[32m[epoch=157/200][train] loss=10.991253\u001b[0m\n",
      "[ 2025-02-07 18:25:00,815][mgbdt.log] \u001b[32m[epoch=158/200][train] loss=10.319420\u001b[0m\n",
      "[ 2025-02-07 18:25:01,829][mgbdt.log] \u001b[32m[epoch=159/200][train] loss=9.964824\u001b[0m\n",
      "[ 2025-02-07 18:25:02,787][mgbdt.log] \u001b[32m[epoch=160/200][train] loss=9.778829\u001b[0m\n",
      "[ 2025-02-07 18:25:03,804][mgbdt.log] \u001b[32m[epoch=161/200][train] loss=9.402000\u001b[0m\n",
      "[ 2025-02-07 18:25:04,825][mgbdt.log] \u001b[32m[epoch=162/200][train] loss=8.969534\u001b[0m\n",
      "[ 2025-02-07 18:25:05,851][mgbdt.log] \u001b[32m[epoch=163/200][train] loss=8.791629\u001b[0m\n",
      "[ 2025-02-07 18:25:06,876][mgbdt.log] \u001b[32m[epoch=164/200][train] loss=8.684754\u001b[0m\n",
      "[ 2025-02-07 18:25:07,912][mgbdt.log] \u001b[32m[epoch=165/200][train] loss=8.271631\u001b[0m\n",
      "[ 2025-02-07 18:25:08,915][mgbdt.log] \u001b[32m[epoch=166/200][train] loss=8.399188\u001b[0m\n",
      "[ 2025-02-07 18:25:09,932][mgbdt.log] \u001b[32m[epoch=167/200][train] loss=8.601268\u001b[0m\n",
      "[ 2025-02-07 18:25:10,943][mgbdt.log] \u001b[32m[epoch=168/200][train] loss=8.357463\u001b[0m\n",
      "[ 2025-02-07 18:25:11,986][mgbdt.log] \u001b[32m[epoch=169/200][train] loss=8.549252\u001b[0m\n",
      "[ 2025-02-07 18:25:13,048][mgbdt.log] \u001b[32m[epoch=170/200][train] loss=8.189307\u001b[0m\n",
      "[ 2025-02-07 18:25:14,090][mgbdt.log] \u001b[32m[epoch=171/200][train] loss=8.634589\u001b[0m\n",
      "[ 2025-02-07 18:25:15,176][mgbdt.log] \u001b[32m[epoch=172/200][train] loss=8.742498\u001b[0m\n",
      "[ 2025-02-07 18:25:16,227][mgbdt.log] \u001b[32m[epoch=173/200][train] loss=8.643779\u001b[0m\n",
      "[ 2025-02-07 18:25:17,263][mgbdt.log] \u001b[32m[epoch=174/200][train] loss=8.970628\u001b[0m\n",
      "[ 2025-02-07 18:25:18,315][mgbdt.log] \u001b[32m[epoch=175/200][train] loss=9.326891\u001b[0m\n",
      "[ 2025-02-07 18:25:19,392][mgbdt.log] \u001b[32m[epoch=176/200][train] loss=8.592772\u001b[0m\n",
      "[ 2025-02-07 18:25:20,477][mgbdt.log] \u001b[32m[epoch=177/200][train] loss=8.776075\u001b[0m\n",
      "[ 2025-02-07 18:25:21,537][mgbdt.log] \u001b[32m[epoch=178/200][train] loss=8.693946\u001b[0m\n",
      "[ 2025-02-07 18:25:22,632][mgbdt.log] \u001b[32m[epoch=179/200][train] loss=8.204373\u001b[0m\n",
      "[ 2025-02-07 18:25:23,725][mgbdt.log] \u001b[32m[epoch=180/200][train] loss=8.366080\u001b[0m\n",
      "[ 2025-02-07 18:25:24,828][mgbdt.log] \u001b[32m[epoch=181/200][train] loss=8.384780\u001b[0m\n",
      "[ 2025-02-07 18:25:25,912][mgbdt.log] \u001b[32m[epoch=182/200][train] loss=8.377028\u001b[0m\n",
      "[ 2025-02-07 18:25:27,009][mgbdt.log] \u001b[32m[epoch=183/200][train] loss=8.537484\u001b[0m\n",
      "[ 2025-02-07 18:25:28,083][mgbdt.log] \u001b[32m[epoch=184/200][train] loss=8.335967\u001b[0m\n",
      "[ 2025-02-07 18:25:29,178][mgbdt.log] \u001b[32m[epoch=185/200][train] loss=8.057555\u001b[0m\n",
      "[ 2025-02-07 18:25:30,287][mgbdt.log] \u001b[32m[epoch=186/200][train] loss=7.958154\u001b[0m\n",
      "[ 2025-02-07 18:25:31,394][mgbdt.log] \u001b[32m[epoch=187/200][train] loss=7.750851\u001b[0m\n",
      "[ 2025-02-07 18:25:32,500][mgbdt.log] \u001b[32m[epoch=188/200][train] loss=7.468729\u001b[0m\n",
      "[ 2025-02-07 18:25:33,647][mgbdt.log] \u001b[32m[epoch=189/200][train] loss=7.285853\u001b[0m\n",
      "[ 2025-02-07 18:25:34,763][mgbdt.log] \u001b[32m[epoch=190/200][train] loss=7.419901\u001b[0m\n",
      "[ 2025-02-07 18:25:35,909][mgbdt.log] \u001b[32m[epoch=191/200][train] loss=7.271963\u001b[0m\n",
      "[ 2025-02-07 18:25:37,051][mgbdt.log] \u001b[32m[epoch=192/200][train] loss=7.507006\u001b[0m\n",
      "[ 2025-02-07 18:25:38,186][mgbdt.log] \u001b[32m[epoch=193/200][train] loss=7.580627\u001b[0m\n",
      "[ 2025-02-07 18:25:39,329][mgbdt.log] \u001b[32m[epoch=194/200][train] loss=7.889531\u001b[0m\n",
      "[ 2025-02-07 18:25:40,482][mgbdt.log] \u001b[32m[epoch=195/200][train] loss=7.149865\u001b[0m\n",
      "[ 2025-02-07 18:25:41,659][mgbdt.log] \u001b[32m[epoch=196/200][train] loss=7.485595\u001b[0m\n",
      "[ 2025-02-07 18:25:42,829][mgbdt.log] \u001b[32m[epoch=197/200][train] loss=7.079292\u001b[0m\n",
      "[ 2025-02-07 18:25:44,013][mgbdt.log] \u001b[32m[epoch=198/200][train] loss=7.167863\u001b[0m\n",
      "[ 2025-02-07 18:25:45,191][mgbdt.log] \u001b[32m[epoch=199/200][train] loss=7.098662\u001b[0m\n",
      "[ 2025-02-07 18:25:46,351][mgbdt.log] \u001b[32m[epoch=200/200][train] loss=7.070032\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "net_linear = MGBDT(loss=None, target_lr=1, epsilon=0.1, verbose=False) \n",
    "\n",
    "# Add target-propogation layers: F, G represent the forward and inverse mapping layers, respectively\n",
    "net_linear.add_layer(\"tp_layer\",\n",
    "                     F=MultiXGBModel(input_size=X_train.shape[1], output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                     G=None)\n",
    "net_linear.add_layer(\"tp_layer\",\n",
    "                     F=MultiXGBModel(input_size=5, output_size=3, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                     G=MultiXGBModel(input_size=3, output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round))\n",
    "net_linear.add_layer(\"bp_layer\",\n",
    "                     F=LinearModel(input_size=3, output_size=1, learning_rate=0.01, loss=loss))\n",
    "\n",
    "\n",
    "# init the forward mapping\n",
    "net_linear.init(X_train, n_rounds=10)\n",
    "\n",
    "net_linear.fit(X_train, y_train.reshape(-1,1), n_epochs=n_epochs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab9690-38ae-4282-b784-6c67d53f06db",
   "metadata": {},
   "source": [
    "## mGBDT XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39719b30-1cfb-4caa-91b1-771f8de06e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[ 2025-02-07 18:25:46,587][mgbdt.log] \u001b[32m[epoch=0/200][train] loss=22.386312\u001b[0m\n",
      "[ 2025-02-07 18:25:47,061][mgbdt.log] \u001b[32m[epoch=1/200][train] loss=22.244833\u001b[0m\n",
      "[ 2025-02-07 18:25:47,512][mgbdt.log] \u001b[32m[epoch=2/200][train] loss=22.115596\u001b[0m\n",
      "[ 2025-02-07 18:25:48,120][mgbdt.log] \u001b[32m[epoch=3/200][train] loss=21.971816\u001b[0m\n",
      "[ 2025-02-07 18:25:48,590][mgbdt.log] \u001b[32m[epoch=4/200][train] loss=21.832660\u001b[0m\n",
      "[ 2025-02-07 18:25:49,040][mgbdt.log] \u001b[32m[epoch=5/200][train] loss=21.691416\u001b[0m\n",
      "[ 2025-02-07 18:25:49,492][mgbdt.log] \u001b[32m[epoch=6/200][train] loss=21.550116\u001b[0m\n",
      "[ 2025-02-07 18:25:49,958][mgbdt.log] \u001b[32m[epoch=7/200][train] loss=21.408705\u001b[0m\n",
      "[ 2025-02-07 18:25:50,451][mgbdt.log] \u001b[32m[epoch=8/200][train] loss=21.266461\u001b[0m\n",
      "[ 2025-02-07 18:25:50,903][mgbdt.log] \u001b[32m[epoch=9/200][train] loss=21.123349\u001b[0m\n",
      "[ 2025-02-07 18:25:51,421][mgbdt.log] \u001b[32m[epoch=10/200][train] loss=20.981255\u001b[0m\n",
      "[ 2025-02-07 18:25:51,928][mgbdt.log] \u001b[32m[epoch=11/200][train] loss=20.841565\u001b[0m\n",
      "[ 2025-02-07 18:25:52,430][mgbdt.log] \u001b[32m[epoch=12/200][train] loss=20.700675\u001b[0m\n",
      "[ 2025-02-07 18:25:52,954][mgbdt.log] \u001b[32m[epoch=13/200][train] loss=20.559715\u001b[0m\n",
      "[ 2025-02-07 18:25:53,461][mgbdt.log] \u001b[32m[epoch=14/200][train] loss=20.419130\u001b[0m\n",
      "[ 2025-02-07 18:25:53,949][mgbdt.log] \u001b[32m[epoch=15/200][train] loss=20.278526\u001b[0m\n",
      "[ 2025-02-07 18:25:54,408][mgbdt.log] \u001b[32m[epoch=16/200][train] loss=20.142461\u001b[0m\n",
      "[ 2025-02-07 18:25:54,929][mgbdt.log] \u001b[32m[epoch=17/200][train] loss=20.025486\u001b[0m\n",
      "[ 2025-02-07 18:25:55,519][mgbdt.log] \u001b[32m[epoch=18/200][train] loss=19.891042\u001b[0m\n",
      "[ 2025-02-07 18:25:56,078][mgbdt.log] \u001b[32m[epoch=19/200][train] loss=19.750361\u001b[0m\n",
      "[ 2025-02-07 18:25:56,646][mgbdt.log] \u001b[32m[epoch=20/200][train] loss=19.571161\u001b[0m\n",
      "[ 2025-02-07 18:25:57,163][mgbdt.log] \u001b[32m[epoch=21/200][train] loss=19.432932\u001b[0m\n",
      "[ 2025-02-07 18:25:57,756][mgbdt.log] \u001b[32m[epoch=22/200][train] loss=19.290672\u001b[0m\n",
      "[ 2025-02-07 18:25:58,305][mgbdt.log] \u001b[32m[epoch=23/200][train] loss=19.149231\u001b[0m\n",
      "[ 2025-02-07 18:25:58,909][mgbdt.log] \u001b[32m[epoch=24/200][train] loss=18.990332\u001b[0m\n",
      "[ 2025-02-07 18:25:59,519][mgbdt.log] \u001b[32m[epoch=25/200][train] loss=18.766175\u001b[0m\n",
      "[ 2025-02-07 18:26:00,188][mgbdt.log] \u001b[32m[epoch=26/200][train] loss=18.623848\u001b[0m\n",
      "[ 2025-02-07 18:26:00,864][mgbdt.log] \u001b[32m[epoch=27/200][train] loss=18.653268\u001b[0m\n",
      "[ 2025-02-07 18:26:01,497][mgbdt.log] \u001b[32m[epoch=28/200][train] loss=18.510942\u001b[0m\n",
      "[ 2025-02-07 18:26:02,090][mgbdt.log] \u001b[32m[epoch=29/200][train] loss=18.182810\u001b[0m\n",
      "[ 2025-02-07 18:26:02,724][mgbdt.log] \u001b[32m[epoch=30/200][train] loss=18.042671\u001b[0m\n",
      "[ 2025-02-07 18:26:03,367][mgbdt.log] \u001b[32m[epoch=31/200][train] loss=17.902737\u001b[0m\n",
      "[ 2025-02-07 18:26:04,012][mgbdt.log] \u001b[32m[epoch=32/200][train] loss=17.761130\u001b[0m\n",
      "[ 2025-02-07 18:26:04,695][mgbdt.log] \u001b[32m[epoch=33/200][train] loss=17.621161\u001b[0m\n",
      "[ 2025-02-07 18:26:05,287][mgbdt.log] \u001b[32m[epoch=34/200][train] loss=17.494816\u001b[0m\n",
      "[ 2025-02-07 18:26:05,998][mgbdt.log] \u001b[32m[epoch=35/200][train] loss=17.480896\u001b[0m\n",
      "[ 2025-02-07 18:26:06,683][mgbdt.log] \u001b[32m[epoch=36/200][train] loss=17.359495\u001b[0m\n",
      "[ 2025-02-07 18:26:07,391][mgbdt.log] \u001b[32m[epoch=37/200][train] loss=17.220812\u001b[0m\n",
      "[ 2025-02-07 18:26:08,077][mgbdt.log] \u001b[32m[epoch=38/200][train] loss=17.091419\u001b[0m\n",
      "[ 2025-02-07 18:26:08,694][mgbdt.log] \u001b[32m[epoch=39/200][train] loss=16.957052\u001b[0m\n",
      "[ 2025-02-07 18:26:09,364][mgbdt.log] \u001b[32m[epoch=40/200][train] loss=16.817929\u001b[0m\n",
      "[ 2025-02-07 18:26:09,991][mgbdt.log] \u001b[32m[epoch=41/200][train] loss=16.683060\u001b[0m\n",
      "[ 2025-02-07 18:26:10,745][mgbdt.log] \u001b[32m[epoch=42/200][train] loss=16.541849\u001b[0m\n",
      "[ 2025-02-07 18:26:11,444][mgbdt.log] \u001b[32m[epoch=43/200][train] loss=16.403833\u001b[0m\n",
      "[ 2025-02-07 18:26:12,176][mgbdt.log] \u001b[32m[epoch=44/200][train] loss=16.261876\u001b[0m\n",
      "[ 2025-02-07 18:26:12,897][mgbdt.log] \u001b[32m[epoch=45/200][train] loss=16.125553\u001b[0m\n",
      "[ 2025-02-07 18:26:13,588][mgbdt.log] \u001b[32m[epoch=46/200][train] loss=15.985997\u001b[0m\n",
      "[ 2025-02-07 18:26:14,273][mgbdt.log] \u001b[32m[epoch=47/200][train] loss=15.860062\u001b[0m\n",
      "[ 2025-02-07 18:26:14,939][mgbdt.log] \u001b[32m[epoch=48/200][train] loss=15.714488\u001b[0m\n",
      "[ 2025-02-07 18:26:15,625][mgbdt.log] \u001b[32m[epoch=49/200][train] loss=15.572463\u001b[0m\n",
      "[ 2025-02-07 18:26:16,303][mgbdt.log] \u001b[32m[epoch=50/200][train] loss=15.448718\u001b[0m\n",
      "[ 2025-02-07 18:26:17,002][mgbdt.log] \u001b[32m[epoch=51/200][train] loss=15.320770\u001b[0m\n",
      "[ 2025-02-07 18:26:17,678][mgbdt.log] \u001b[32m[epoch=52/200][train] loss=15.192237\u001b[0m\n",
      "[ 2025-02-07 18:26:18,388][mgbdt.log] \u001b[32m[epoch=53/200][train] loss=15.063332\u001b[0m\n",
      "[ 2025-02-07 18:26:19,088][mgbdt.log] \u001b[32m[epoch=54/200][train] loss=14.920286\u001b[0m\n",
      "[ 2025-02-07 18:26:19,756][mgbdt.log] \u001b[32m[epoch=55/200][train] loss=14.792911\u001b[0m\n",
      "[ 2025-02-07 18:26:20,450][mgbdt.log] \u001b[32m[epoch=56/200][train] loss=14.658553\u001b[0m\n",
      "[ 2025-02-07 18:26:21,126][mgbdt.log] \u001b[32m[epoch=57/200][train] loss=14.516773\u001b[0m\n",
      "[ 2025-02-07 18:26:21,827][mgbdt.log] \u001b[32m[epoch=58/200][train] loss=14.393218\u001b[0m\n",
      "[ 2025-02-07 18:26:22,520][mgbdt.log] \u001b[32m[epoch=59/200][train] loss=14.255771\u001b[0m\n",
      "[ 2025-02-07 18:26:23,205][mgbdt.log] \u001b[32m[epoch=60/200][train] loss=14.126993\u001b[0m\n",
      "[ 2025-02-07 18:26:23,931][mgbdt.log] \u001b[32m[epoch=61/200][train] loss=14.002153\u001b[0m\n",
      "[ 2025-02-07 18:26:24,686][mgbdt.log] \u001b[32m[epoch=62/200][train] loss=13.887030\u001b[0m\n",
      "[ 2025-02-07 18:26:25,393][mgbdt.log] \u001b[32m[epoch=63/200][train] loss=13.753517\u001b[0m\n",
      "[ 2025-02-07 18:26:26,147][mgbdt.log] \u001b[32m[epoch=64/200][train] loss=13.618720\u001b[0m\n",
      "[ 2025-02-07 18:26:26,829][mgbdt.log] \u001b[32m[epoch=65/200][train] loss=13.497484\u001b[0m\n",
      "[ 2025-02-07 18:26:27,547][mgbdt.log] \u001b[32m[epoch=66/200][train] loss=13.371484\u001b[0m\n",
      "[ 2025-02-07 18:26:28,290][mgbdt.log] \u001b[32m[epoch=67/200][train] loss=13.239634\u001b[0m\n",
      "[ 2025-02-07 18:26:29,024][mgbdt.log] \u001b[32m[epoch=68/200][train] loss=13.137515\u001b[0m\n",
      "[ 2025-02-07 18:26:29,759][mgbdt.log] \u001b[32m[epoch=69/200][train] loss=13.046351\u001b[0m\n",
      "[ 2025-02-07 18:26:30,469][mgbdt.log] \u001b[32m[epoch=70/200][train] loss=12.916294\u001b[0m\n",
      "[ 2025-02-07 18:26:31,187][mgbdt.log] \u001b[32m[epoch=71/200][train] loss=12.794231\u001b[0m\n",
      "[ 2025-02-07 18:26:31,922][mgbdt.log] \u001b[32m[epoch=72/200][train] loss=12.711158\u001b[0m\n",
      "[ 2025-02-07 18:26:32,675][mgbdt.log] \u001b[32m[epoch=73/200][train] loss=12.641458\u001b[0m\n",
      "[ 2025-02-07 18:26:33,417][mgbdt.log] \u001b[32m[epoch=74/200][train] loss=12.627063\u001b[0m\n",
      "[ 2025-02-07 18:26:34,217][mgbdt.log] \u001b[32m[epoch=75/200][train] loss=12.463455\u001b[0m\n",
      "[ 2025-02-07 18:26:35,010][mgbdt.log] \u001b[32m[epoch=76/200][train] loss=12.326325\u001b[0m\n",
      "[ 2025-02-07 18:26:35,766][mgbdt.log] \u001b[32m[epoch=77/200][train] loss=12.184788\u001b[0m\n",
      "[ 2025-02-07 18:26:36,546][mgbdt.log] \u001b[32m[epoch=78/200][train] loss=12.071223\u001b[0m\n",
      "[ 2025-02-07 18:26:37,340][mgbdt.log] \u001b[32m[epoch=79/200][train] loss=11.982561\u001b[0m\n",
      "[ 2025-02-07 18:26:38,140][mgbdt.log] \u001b[32m[epoch=80/200][train] loss=11.896054\u001b[0m\n",
      "[ 2025-02-07 18:26:38,943][mgbdt.log] \u001b[32m[epoch=81/200][train] loss=11.790116\u001b[0m\n",
      "[ 2025-02-07 18:26:39,728][mgbdt.log] \u001b[32m[epoch=82/200][train] loss=11.735536\u001b[0m\n",
      "[ 2025-02-07 18:26:40,563][mgbdt.log] \u001b[32m[epoch=83/200][train] loss=11.644173\u001b[0m\n",
      "[ 2025-02-07 18:26:41,365][mgbdt.log] \u001b[32m[epoch=84/200][train] loss=11.540480\u001b[0m\n",
      "[ 2025-02-07 18:26:42,191][mgbdt.log] \u001b[32m[epoch=85/200][train] loss=11.430528\u001b[0m\n",
      "[ 2025-02-07 18:26:43,018][mgbdt.log] \u001b[32m[epoch=86/200][train] loss=11.290667\u001b[0m\n",
      "[ 2025-02-07 18:26:43,904][mgbdt.log] \u001b[32m[epoch=87/200][train] loss=11.167574\u001b[0m\n",
      "[ 2025-02-07 18:26:44,837][mgbdt.log] \u001b[32m[epoch=88/200][train] loss=11.075595\u001b[0m\n",
      "[ 2025-02-07 18:26:45,739][mgbdt.log] \u001b[32m[epoch=89/200][train] loss=10.988423\u001b[0m\n",
      "[ 2025-02-07 18:26:46,675][mgbdt.log] \u001b[32m[epoch=90/200][train] loss=10.893315\u001b[0m\n",
      "[ 2025-02-07 18:26:47,558][mgbdt.log] \u001b[32m[epoch=91/200][train] loss=10.825957\u001b[0m\n",
      "[ 2025-02-07 18:26:48,521][mgbdt.log] \u001b[32m[epoch=92/200][train] loss=10.838775\u001b[0m\n",
      "[ 2025-02-07 18:26:49,471][mgbdt.log] \u001b[32m[epoch=93/200][train] loss=10.727309\u001b[0m\n",
      "[ 2025-02-07 18:26:50,456][mgbdt.log] \u001b[32m[epoch=94/200][train] loss=10.718760\u001b[0m\n",
      "[ 2025-02-07 18:26:51,467][mgbdt.log] \u001b[32m[epoch=95/200][train] loss=10.708855\u001b[0m\n",
      "[ 2025-02-07 18:26:52,477][mgbdt.log] \u001b[32m[epoch=96/200][train] loss=10.748463\u001b[0m\n",
      "[ 2025-02-07 18:26:53,546][mgbdt.log] \u001b[32m[epoch=97/200][train] loss=10.605337\u001b[0m\n",
      "[ 2025-02-07 18:26:54,590][mgbdt.log] \u001b[32m[epoch=98/200][train] loss=10.490774\u001b[0m\n",
      "[ 2025-02-07 18:26:55,590][mgbdt.log] \u001b[32m[epoch=99/200][train] loss=10.365527\u001b[0m\n",
      "[ 2025-02-07 18:26:56,635][mgbdt.log] \u001b[32m[epoch=100/200][train] loss=10.261726\u001b[0m\n",
      "[ 2025-02-07 18:26:57,670][mgbdt.log] \u001b[32m[epoch=101/200][train] loss=10.362604\u001b[0m\n",
      "[ 2025-02-07 18:26:58,697][mgbdt.log] \u001b[32m[epoch=102/200][train] loss=10.112315\u001b[0m\n",
      "[ 2025-02-07 18:26:59,757][mgbdt.log] \u001b[32m[epoch=103/200][train] loss=9.792754\u001b[0m\n",
      "[ 2025-02-07 18:27:00,843][mgbdt.log] \u001b[32m[epoch=104/200][train] loss=9.637246\u001b[0m\n",
      "[ 2025-02-07 18:27:01,853][mgbdt.log] \u001b[32m[epoch=105/200][train] loss=9.630479\u001b[0m\n",
      "[ 2025-02-07 18:27:02,936][mgbdt.log] \u001b[32m[epoch=106/200][train] loss=9.575370\u001b[0m\n",
      "[ 2025-02-07 18:27:03,985][mgbdt.log] \u001b[32m[epoch=107/200][train] loss=9.416548\u001b[0m\n",
      "[ 2025-02-07 18:27:05,084][mgbdt.log] \u001b[32m[epoch=108/200][train] loss=9.303061\u001b[0m\n",
      "[ 2025-02-07 18:27:06,112][mgbdt.log] \u001b[32m[epoch=109/200][train] loss=9.210055\u001b[0m\n",
      "[ 2025-02-07 18:27:07,213][mgbdt.log] \u001b[32m[epoch=110/200][train] loss=9.171032\u001b[0m\n",
      "[ 2025-02-07 18:27:08,282][mgbdt.log] \u001b[32m[epoch=111/200][train] loss=9.137630\u001b[0m\n",
      "[ 2025-02-07 18:27:09,337][mgbdt.log] \u001b[32m[epoch=112/200][train] loss=9.032543\u001b[0m\n",
      "[ 2025-02-07 18:27:10,437][mgbdt.log] \u001b[32m[epoch=113/200][train] loss=8.987083\u001b[0m\n",
      "[ 2025-02-07 18:27:11,513][mgbdt.log] \u001b[32m[epoch=114/200][train] loss=8.927708\u001b[0m\n",
      "[ 2025-02-07 18:27:12,614][mgbdt.log] \u001b[32m[epoch=115/200][train] loss=8.924297\u001b[0m\n",
      "[ 2025-02-07 18:27:13,734][mgbdt.log] \u001b[32m[epoch=116/200][train] loss=8.908702\u001b[0m\n",
      "[ 2025-02-07 18:27:14,836][mgbdt.log] \u001b[32m[epoch=117/200][train] loss=8.873494\u001b[0m\n",
      "[ 2025-02-07 18:27:15,960][mgbdt.log] \u001b[32m[epoch=118/200][train] loss=8.856735\u001b[0m\n",
      "[ 2025-02-07 18:27:17,072][mgbdt.log] \u001b[32m[epoch=119/200][train] loss=8.807728\u001b[0m\n",
      "[ 2025-02-07 18:27:18,234][mgbdt.log] \u001b[32m[epoch=120/200][train] loss=8.814148\u001b[0m\n",
      "[ 2025-02-07 18:27:19,353][mgbdt.log] \u001b[32m[epoch=121/200][train] loss=8.823553\u001b[0m\n",
      "[ 2025-02-07 18:27:20,478][mgbdt.log] \u001b[32m[epoch=122/200][train] loss=8.833146\u001b[0m\n",
      "[ 2025-02-07 18:27:21,601][mgbdt.log] \u001b[32m[epoch=123/200][train] loss=8.863368\u001b[0m\n",
      "[ 2025-02-07 18:27:22,784][mgbdt.log] \u001b[32m[epoch=124/200][train] loss=8.777689\u001b[0m\n",
      "[ 2025-02-07 18:27:23,953][mgbdt.log] \u001b[32m[epoch=125/200][train] loss=8.714922\u001b[0m\n",
      "[ 2025-02-07 18:27:25,099][mgbdt.log] \u001b[32m[epoch=126/200][train] loss=8.663695\u001b[0m\n",
      "[ 2025-02-07 18:27:26,256][mgbdt.log] \u001b[32m[epoch=127/200][train] loss=8.589647\u001b[0m\n",
      "[ 2025-02-07 18:27:27,473][mgbdt.log] \u001b[32m[epoch=128/200][train] loss=8.678093\u001b[0m\n",
      "[ 2025-02-07 18:27:28,669][mgbdt.log] \u001b[32m[epoch=129/200][train] loss=8.593462\u001b[0m\n",
      "[ 2025-02-07 18:27:29,790][mgbdt.log] \u001b[32m[epoch=130/200][train] loss=8.671023\u001b[0m\n",
      "[ 2025-02-07 18:27:31,017][mgbdt.log] \u001b[32m[epoch=131/200][train] loss=8.596091\u001b[0m\n",
      "[ 2025-02-07 18:27:32,077][mgbdt.log] \u001b[32m[epoch=132/200][train] loss=8.493776\u001b[0m\n",
      "[ 2025-02-07 18:27:33,162][mgbdt.log] \u001b[32m[epoch=133/200][train] loss=8.417276\u001b[0m\n",
      "[ 2025-02-07 18:27:34,206][mgbdt.log] \u001b[32m[epoch=134/200][train] loss=8.370887\u001b[0m\n",
      "[ 2025-02-07 18:27:35,289][mgbdt.log] \u001b[32m[epoch=135/200][train] loss=8.316870\u001b[0m\n",
      "[ 2025-02-07 18:27:36,503][mgbdt.log] \u001b[32m[epoch=136/200][train] loss=8.228374\u001b[0m\n",
      "[ 2025-02-07 18:27:37,601][mgbdt.log] \u001b[32m[epoch=137/200][train] loss=8.266471\u001b[0m\n",
      "[ 2025-02-07 18:27:38,730][mgbdt.log] \u001b[32m[epoch=138/200][train] loss=8.243290\u001b[0m\n",
      "[ 2025-02-07 18:27:39,839][mgbdt.log] \u001b[32m[epoch=139/200][train] loss=8.318485\u001b[0m\n",
      "[ 2025-02-07 18:27:40,953][mgbdt.log] \u001b[32m[epoch=140/200][train] loss=8.276495\u001b[0m\n",
      "[ 2025-02-07 18:27:42,093][mgbdt.log] \u001b[32m[epoch=141/200][train] loss=8.241490\u001b[0m\n",
      "[ 2025-02-07 18:27:43,274][mgbdt.log] \u001b[32m[epoch=142/200][train] loss=8.121446\u001b[0m\n",
      "[ 2025-02-07 18:27:44,401][mgbdt.log] \u001b[32m[epoch=143/200][train] loss=8.020312\u001b[0m\n",
      "[ 2025-02-07 18:27:45,527][mgbdt.log] \u001b[32m[epoch=144/200][train] loss=7.984594\u001b[0m\n",
      "[ 2025-02-07 18:27:46,655][mgbdt.log] \u001b[32m[epoch=145/200][train] loss=7.852534\u001b[0m\n",
      "[ 2025-02-07 18:27:47,765][mgbdt.log] \u001b[32m[epoch=146/200][train] loss=7.799202\u001b[0m\n",
      "[ 2025-02-07 18:27:49,121][mgbdt.log] \u001b[32m[epoch=147/200][train] loss=7.758535\u001b[0m\n",
      "[ 2025-02-07 18:27:50,487][mgbdt.log] \u001b[32m[epoch=148/200][train] loss=7.671199\u001b[0m\n",
      "[ 2025-02-07 18:27:51,860][mgbdt.log] \u001b[32m[epoch=149/200][train] loss=7.599031\u001b[0m\n",
      "[ 2025-02-07 18:27:53,218][mgbdt.log] \u001b[32m[epoch=150/200][train] loss=7.487844\u001b[0m\n",
      "[ 2025-02-07 18:27:54,595][mgbdt.log] \u001b[32m[epoch=151/200][train] loss=7.526019\u001b[0m\n",
      "[ 2025-02-07 18:27:55,988][mgbdt.log] \u001b[32m[epoch=152/200][train] loss=7.456808\u001b[0m\n",
      "[ 2025-02-07 18:27:57,408][mgbdt.log] \u001b[32m[epoch=153/200][train] loss=7.589294\u001b[0m\n",
      "[ 2025-02-07 18:27:58,820][mgbdt.log] \u001b[32m[epoch=154/200][train] loss=7.532414\u001b[0m\n",
      "[ 2025-02-07 18:28:00,198][mgbdt.log] \u001b[32m[epoch=155/200][train] loss=7.394507\u001b[0m\n",
      "[ 2025-02-07 18:28:01,578][mgbdt.log] \u001b[32m[epoch=156/200][train] loss=7.315030\u001b[0m\n",
      "[ 2025-02-07 18:28:02,961][mgbdt.log] \u001b[32m[epoch=157/200][train] loss=7.210039\u001b[0m\n",
      "[ 2025-02-07 18:28:04,397][mgbdt.log] \u001b[32m[epoch=158/200][train] loss=7.066309\u001b[0m\n",
      "[ 2025-02-07 18:28:05,792][mgbdt.log] \u001b[32m[epoch=159/200][train] loss=7.040883\u001b[0m\n",
      "[ 2025-02-07 18:28:07,211][mgbdt.log] \u001b[32m[epoch=160/200][train] loss=6.978171\u001b[0m\n",
      "[ 2025-02-07 18:28:08,654][mgbdt.log] \u001b[32m[epoch=161/200][train] loss=6.997793\u001b[0m\n",
      "[ 2025-02-07 18:28:10,099][mgbdt.log] \u001b[32m[epoch=162/200][train] loss=7.083482\u001b[0m\n",
      "[ 2025-02-07 18:28:11,553][mgbdt.log] \u001b[32m[epoch=163/200][train] loss=7.036178\u001b[0m\n",
      "[ 2025-02-07 18:28:13,071][mgbdt.log] \u001b[32m[epoch=164/200][train] loss=7.018705\u001b[0m\n",
      "[ 2025-02-07 18:28:14,575][mgbdt.log] \u001b[32m[epoch=165/200][train] loss=7.023129\u001b[0m\n",
      "[ 2025-02-07 18:28:16,034][mgbdt.log] \u001b[32m[epoch=166/200][train] loss=7.028324\u001b[0m\n",
      "[ 2025-02-07 18:28:17,464][mgbdt.log] \u001b[32m[epoch=167/200][train] loss=7.030326\u001b[0m\n",
      "[ 2025-02-07 18:28:18,950][mgbdt.log] \u001b[32m[epoch=168/200][train] loss=7.064919\u001b[0m\n",
      "[ 2025-02-07 18:28:20,421][mgbdt.log] \u001b[32m[epoch=169/200][train] loss=7.049660\u001b[0m\n",
      "[ 2025-02-07 18:28:21,939][mgbdt.log] \u001b[32m[epoch=170/200][train] loss=7.063492\u001b[0m\n",
      "[ 2025-02-07 18:28:23,492][mgbdt.log] \u001b[32m[epoch=171/200][train] loss=7.042650\u001b[0m\n",
      "[ 2025-02-07 18:28:24,953][mgbdt.log] \u001b[32m[epoch=172/200][train] loss=7.032746\u001b[0m\n",
      "[ 2025-02-07 18:28:26,480][mgbdt.log] \u001b[32m[epoch=173/200][train] loss=7.055045\u001b[0m\n",
      "[ 2025-02-07 18:28:27,983][mgbdt.log] \u001b[32m[epoch=174/200][train] loss=7.015406\u001b[0m\n",
      "[ 2025-02-07 18:28:29,499][mgbdt.log] \u001b[32m[epoch=175/200][train] loss=7.035709\u001b[0m\n",
      "[ 2025-02-07 18:28:31,056][mgbdt.log] \u001b[32m[epoch=176/200][train] loss=7.045938\u001b[0m\n",
      "[ 2025-02-07 18:28:32,613][mgbdt.log] \u001b[32m[epoch=177/200][train] loss=7.069194\u001b[0m\n",
      "[ 2025-02-07 18:28:34,212][mgbdt.log] \u001b[32m[epoch=178/200][train] loss=7.085645\u001b[0m\n",
      "[ 2025-02-07 18:28:35,731][mgbdt.log] \u001b[32m[epoch=179/200][train] loss=7.128594\u001b[0m\n",
      "[ 2025-02-07 18:28:37,325][mgbdt.log] \u001b[32m[epoch=180/200][train] loss=7.116460\u001b[0m\n",
      "[ 2025-02-07 18:28:38,937][mgbdt.log] \u001b[32m[epoch=181/200][train] loss=7.065722\u001b[0m\n",
      "[ 2025-02-07 18:28:40,516][mgbdt.log] \u001b[32m[epoch=182/200][train] loss=7.083756\u001b[0m\n",
      "[ 2025-02-07 18:28:42,122][mgbdt.log] \u001b[32m[epoch=183/200][train] loss=6.982204\u001b[0m\n",
      "[ 2025-02-07 18:28:43,729][mgbdt.log] \u001b[32m[epoch=184/200][train] loss=6.851925\u001b[0m\n",
      "[ 2025-02-07 18:28:45,367][mgbdt.log] \u001b[32m[epoch=185/200][train] loss=6.781333\u001b[0m\n",
      "[ 2025-02-07 18:28:46,735][mgbdt.log] \u001b[32m[epoch=186/200][train] loss=6.821731\u001b[0m\n",
      "[ 2025-02-07 18:28:48,155][mgbdt.log] \u001b[32m[epoch=187/200][train] loss=6.844384\u001b[0m\n",
      "[ 2025-02-07 18:28:49,573][mgbdt.log] \u001b[32m[epoch=188/200][train] loss=6.874520\u001b[0m\n",
      "[ 2025-02-07 18:28:51,012][mgbdt.log] \u001b[32m[epoch=189/200][train] loss=7.116458\u001b[0m\n",
      "[ 2025-02-07 18:28:52,498][mgbdt.log] \u001b[32m[epoch=190/200][train] loss=7.073210\u001b[0m\n",
      "[ 2025-02-07 18:28:53,886][mgbdt.log] \u001b[32m[epoch=191/200][train] loss=7.028742\u001b[0m\n",
      "[ 2025-02-07 18:28:55,267][mgbdt.log] \u001b[32m[epoch=192/200][train] loss=6.926417\u001b[0m\n",
      "[ 2025-02-07 18:28:56,719][mgbdt.log] \u001b[32m[epoch=193/200][train] loss=6.800723\u001b[0m\n",
      "[ 2025-02-07 18:28:58,116][mgbdt.log] \u001b[32m[epoch=194/200][train] loss=6.725124\u001b[0m\n",
      "[ 2025-02-07 18:28:59,611][mgbdt.log] \u001b[32m[epoch=195/200][train] loss=6.632468\u001b[0m\n",
      "[ 2025-02-07 18:29:01,080][mgbdt.log] \u001b[32m[epoch=196/200][train] loss=6.403380\u001b[0m\n",
      "[ 2025-02-07 18:29:02,534][mgbdt.log] \u001b[32m[epoch=197/200][train] loss=6.241202\u001b[0m\n",
      "[ 2025-02-07 18:29:04,078][mgbdt.log] \u001b[32m[epoch=198/200][train] loss=6.060110\u001b[0m\n",
      "[ 2025-02-07 18:29:05,664][mgbdt.log] \u001b[32m[epoch=199/200][train] loss=5.952709\u001b[0m\n",
      "[ 2025-02-07 18:29:07,217][mgbdt.log] \u001b[32m[epoch=200/200][train] loss=5.904739\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "net_xgboost = MGBDT(loss=loss, target_lr=1, epsilon=0.1, verbose=False) \n",
    "\n",
    "# Add target-propogation layers: F, G represent the forward and inverse mapping layers, respectively\n",
    "net_xgboost.add_layer(\"tp_layer\",\n",
    "                      F=MultiXGBModel(input_size=X_train.shape[1], output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                      G=None)\n",
    "net_xgboost.add_layer(\"tp_layer\",\n",
    "                      F=MultiXGBModel(input_size=5, output_size=3, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                      G=MultiXGBModel(input_size=3, output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round))\n",
    "net_xgboost.add_layer(\"tp_layer\",\n",
    "                      F=MultiXGBModel(input_size=3, output_size=1, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                      G=MultiXGBModel(input_size=1, output_size=3, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round))\n",
    "\n",
    "\n",
    "# init the forward mapping\n",
    "net_xgboost.init(X_train, n_rounds=10)\n",
    "\n",
    "net_xgboost.fit(X_train, y_train.reshape(-1,1), n_epochs=n_epochs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a53254-d84a-4a79-bac7-e07db2fe0be4",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d63cd9-e177-4366-a2f3-ab5aefee0a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\torch\\nn\\modules\\loss.py:128: UserWarning: Using a target size (torch.Size([354])) that is different to the input size (torch.Size([354, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "linear_model = LinearModel(input_size=13, output_size=1, loss=loss)\n",
    "for _ in range(n_epochs):\n",
    "    linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbd3d3-ff95-48aa-9e12-fdd311f58a13",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ad1898-e881-4eda-8819-3e0c4b2e0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"eta\": learning_rate,                   \n",
    "          \"max_depth\": max_depth\n",
    "         }\n",
    "\n",
    "xgb_model = xgb.train(params,\n",
    "                      dtrain,\n",
    "                      num_boost_round=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3badc93-4d9d-4fa4-8a32-e76849ffd53d",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61906a88-81d3-47c5-b1d5-efaffb73d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mgbdt_linear = pd.DataFrame.from_dict({\"model\": \"mGBDT_Linear\",\n",
    "                                            \"pred\": net_linear.forward(X_test).reshape(-1,),\n",
    "                                            \"y\": y_test\n",
    "                                           })\n",
    "\n",
    "pred_mgbdt_xgboost = pd.DataFrame.from_dict({\"model\": \"mGBDT_XGBoost\", \n",
    "                                             \"pred\": net_xgboost.forward(X_test).reshape(-1,),\n",
    "                                             \"y\": y_test\n",
    "                                            })\n",
    "\n",
    "pred_linear = pd.DataFrame.from_dict({\"model\": \"Linear\", \n",
    "                                      \"pred\": linear_model.predict(X_test).reshape(-1,),\n",
    "                                      \"y\": y_test\n",
    "                                     })\n",
    "\n",
    "\n",
    "pred_xgboost = pd.DataFrame.from_dict({\"model\": \"XGBoost\",\n",
    "                                       \"pred\": xgb_model.predict(dtest),\n",
    "                                       \"y\": y_test\n",
    "                                      })\n",
    "\n",
    "pred_df = pd.concat([pred_mgbdt_linear, pred_mgbdt_xgboost, pred_linear, pred_xgboost])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7e835-b89c-40c5-9903-95406a846768",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9054fa3-bec4-43b3-8e6c-2702a407d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_45052\\1387413843.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  pred_df.groupby(\"model\").apply(lambda x: mean_absolute_error(y_true=x.y, y_pred=x.pred)).reset_index().rename(columns={0: \"MAE\"}).sort_values(by=\"MAE\").reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>2.377509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mGBDT_Linear</td>\n",
       "      <td>3.703030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mGBDT_XGBoost</td>\n",
       "      <td>6.272753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear</td>\n",
       "      <td>72.753305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model        MAE\n",
       "0        XGBoost   2.377509\n",
       "1   mGBDT_Linear   3.703030\n",
       "2  mGBDT_XGBoost   6.272753\n",
       "3         Linear  72.753305"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.groupby(\"model\").apply(lambda x: mean_absolute_error(y_true=x.y, y_pred=x.pred)).reset_index().rename(columns={0: \"MAE\"}).sort_values(by=\"MAE\").reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
