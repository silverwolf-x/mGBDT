{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1866f89-a655-4a39-99d5-bec2bdd132f9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37177014-7e79-423d-b5b8-be4bdce512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import os.path as osp\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "# For using the mgbdt library, you have to include the library directory into your python path.\n",
    "# If you are in this repository's root directory, you can do it by using the following lines\n",
    "import sys,os\n",
    "# sys.path.insert(0, os.path.join('mGBDT','lib'))\n",
    "sys.path.insert(0, os.path.join('lib'))\n",
    "\n",
    "from mgbdt import MGBDT\n",
    "from mgbdt import MultiXGBModel, LinearModel\n",
    "from mgbdt.utils.plot_utils import plot2d, plot3d\n",
    "from mgbdt.utils.exp_utils import set_seed\n",
    "from mgbdt.utils.log_utils import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07f372-c724-40c8-b169-31caa63ea952",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34230c15-4335-4048-8fbf-4e8d21af75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boston = datasets.load_boston()\n",
    "# X, y = boston.data, boston.target\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "x_mean = X.mean(axis=0)\n",
    "x_std = X.std(axis=0)\n",
    "X = (X - x_mean) / x_std\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df76023-b8a0-4292-b47a-f616645006dc",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce9f009-e146-4097-b781-b4002ccb642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "num_boost_round = 5\n",
    "learning_rate = 0.03\n",
    "max_depth = 5\n",
    "loss = \"L1Loss\"\n",
    "\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda6fc8-6a3b-4d2e-955c-3345bf8879fe",
   "metadata": {},
   "source": [
    "## mGBDT Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9753dd5-2cf6-4a79-a761-e0a80c00b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\xgboost\\core.py:613: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\n",
      "[ 2025-02-08 15:42:27,281][mgbdt.log] \u001b[32m[epoch=0/200][train] loss=602.797764\u001b[0m\n",
      "[ 2025-02-08 15:42:27,704][mgbdt.log] \u001b[32m[epoch=1/200][train] loss=601.589081\u001b[0m\n",
      "[ 2025-02-08 15:42:28,109][mgbdt.log] \u001b[32m[epoch=2/200][train] loss=600.068914\u001b[0m\n",
      "[ 2025-02-08 15:42:28,514][mgbdt.log] \u001b[32m[epoch=3/200][train] loss=598.612313\u001b[0m\n",
      "[ 2025-02-08 15:42:28,916][mgbdt.log] \u001b[32m[epoch=4/200][train] loss=597.232585\u001b[0m\n",
      "[ 2025-02-08 15:42:29,319][mgbdt.log] \u001b[32m[epoch=5/200][train] loss=595.811719\u001b[0m\n",
      "[ 2025-02-08 15:42:29,724][mgbdt.log] \u001b[32m[epoch=6/200][train] loss=594.468897\u001b[0m\n",
      "[ 2025-02-08 15:42:30,130][mgbdt.log] \u001b[32m[epoch=7/200][train] loss=593.027381\u001b[0m\n",
      "[ 2025-02-08 15:42:30,533][mgbdt.log] \u001b[32m[epoch=8/200][train] loss=591.632477\u001b[0m\n",
      "[ 2025-02-08 15:42:30,938][mgbdt.log] \u001b[32m[epoch=9/200][train] loss=590.312126\u001b[0m\n",
      "[ 2025-02-08 15:42:31,345][mgbdt.log] \u001b[32m[epoch=10/200][train] loss=588.995318\u001b[0m\n",
      "[ 2025-02-08 15:42:31,777][mgbdt.log] \u001b[32m[epoch=11/200][train] loss=587.706288\u001b[0m\n",
      "[ 2025-02-08 15:42:32,353][mgbdt.log] \u001b[32m[epoch=12/200][train] loss=586.370398\u001b[0m\n",
      "[ 2025-02-08 15:42:32,792][mgbdt.log] \u001b[32m[epoch=13/200][train] loss=585.015023\u001b[0m\n",
      "[ 2025-02-08 15:42:33,225][mgbdt.log] \u001b[32m[epoch=14/200][train] loss=583.646347\u001b[0m\n",
      "[ 2025-02-08 15:42:33,661][mgbdt.log] \u001b[32m[epoch=15/200][train] loss=582.284524\u001b[0m\n",
      "[ 2025-02-08 15:42:34,100][mgbdt.log] \u001b[32m[epoch=16/200][train] loss=580.879557\u001b[0m\n",
      "[ 2025-02-08 15:42:34,533][mgbdt.log] \u001b[32m[epoch=17/200][train] loss=579.453250\u001b[0m\n",
      "[ 2025-02-08 15:42:34,986][mgbdt.log] \u001b[32m[epoch=18/200][train] loss=578.033564\u001b[0m\n",
      "[ 2025-02-08 15:42:35,423][mgbdt.log] \u001b[32m[epoch=19/200][train] loss=576.535249\u001b[0m\n",
      "[ 2025-02-08 15:42:35,871][mgbdt.log] \u001b[32m[epoch=20/200][train] loss=575.036874\u001b[0m\n",
      "[ 2025-02-08 15:42:36,323][mgbdt.log] \u001b[32m[epoch=21/200][train] loss=573.452481\u001b[0m\n",
      "[ 2025-02-08 15:42:36,789][mgbdt.log] \u001b[32m[epoch=22/200][train] loss=571.785763\u001b[0m\n",
      "[ 2025-02-08 15:42:37,258][mgbdt.log] \u001b[32m[epoch=23/200][train] loss=570.040272\u001b[0m\n",
      "[ 2025-02-08 15:42:37,738][mgbdt.log] \u001b[32m[epoch=24/200][train] loss=568.235959\u001b[0m\n",
      "[ 2025-02-08 15:42:38,222][mgbdt.log] \u001b[32m[epoch=25/200][train] loss=566.287353\u001b[0m\n",
      "[ 2025-02-08 15:42:38,732][mgbdt.log] \u001b[32m[epoch=26/200][train] loss=564.247364\u001b[0m\n",
      "[ 2025-02-08 15:42:39,197][mgbdt.log] \u001b[32m[epoch=27/200][train] loss=562.076396\u001b[0m\n",
      "[ 2025-02-08 15:42:39,667][mgbdt.log] \u001b[32m[epoch=28/200][train] loss=559.792177\u001b[0m\n",
      "[ 2025-02-08 15:42:40,163][mgbdt.log] \u001b[32m[epoch=29/200][train] loss=557.411929\u001b[0m\n",
      "[ 2025-02-08 15:42:40,627][mgbdt.log] \u001b[32m[epoch=30/200][train] loss=554.894857\u001b[0m\n",
      "[ 2025-02-08 15:42:41,125][mgbdt.log] \u001b[32m[epoch=31/200][train] loss=552.227670\u001b[0m\n",
      "[ 2025-02-08 15:42:41,626][mgbdt.log] \u001b[32m[epoch=32/200][train] loss=549.385412\u001b[0m\n",
      "[ 2025-02-08 15:42:42,127][mgbdt.log] \u001b[32m[epoch=33/200][train] loss=546.314238\u001b[0m\n",
      "[ 2025-02-08 15:42:42,592][mgbdt.log] \u001b[32m[epoch=34/200][train] loss=542.990758\u001b[0m\n",
      "[ 2025-02-08 15:42:43,073][mgbdt.log] \u001b[32m[epoch=35/200][train] loss=539.568508\u001b[0m\n",
      "[ 2025-02-08 15:42:43,571][mgbdt.log] \u001b[32m[epoch=36/200][train] loss=535.956399\u001b[0m\n",
      "[ 2025-02-08 15:42:44,116][mgbdt.log] \u001b[32m[epoch=37/200][train] loss=532.229942\u001b[0m\n",
      "[ 2025-02-08 15:42:44,671][mgbdt.log] \u001b[32m[epoch=38/200][train] loss=528.406322\u001b[0m\n",
      "[ 2025-02-08 15:42:45,213][mgbdt.log] \u001b[32m[epoch=39/200][train] loss=524.398429\u001b[0m\n",
      "[ 2025-02-08 15:42:45,772][mgbdt.log] \u001b[32m[epoch=40/200][train] loss=520.228475\u001b[0m\n",
      "[ 2025-02-08 15:42:46,283][mgbdt.log] \u001b[32m[epoch=41/200][train] loss=516.143658\u001b[0m\n",
      "[ 2025-02-08 15:42:46,797][mgbdt.log] \u001b[32m[epoch=42/200][train] loss=511.473419\u001b[0m\n",
      "[ 2025-02-08 15:42:47,391][mgbdt.log] \u001b[32m[epoch=43/200][train] loss=506.414483\u001b[0m\n",
      "[ 2025-02-08 15:42:47,934][mgbdt.log] \u001b[32m[epoch=44/200][train] loss=501.049674\u001b[0m\n",
      "[ 2025-02-08 15:42:48,491][mgbdt.log] \u001b[32m[epoch=45/200][train] loss=495.337758\u001b[0m\n",
      "[ 2025-02-08 15:42:49,034][mgbdt.log] \u001b[32m[epoch=46/200][train] loss=489.232966\u001b[0m\n",
      "[ 2025-02-08 15:42:49,597][mgbdt.log] \u001b[32m[epoch=47/200][train] loss=482.636168\u001b[0m\n",
      "[ 2025-02-08 15:42:50,156][mgbdt.log] \u001b[32m[epoch=48/200][train] loss=475.601004\u001b[0m\n",
      "[ 2025-02-08 15:42:50,712][mgbdt.log] \u001b[32m[epoch=49/200][train] loss=468.027769\u001b[0m\n",
      "[ 2025-02-08 15:42:51,256][mgbdt.log] \u001b[32m[epoch=50/200][train] loss=460.092563\u001b[0m\n",
      "[ 2025-02-08 15:42:51,799][mgbdt.log] \u001b[32m[epoch=51/200][train] loss=451.669883\u001b[0m\n",
      "[ 2025-02-08 15:42:52,362][mgbdt.log] \u001b[32m[epoch=52/200][train] loss=442.036485\u001b[0m\n",
      "[ 2025-02-08 15:42:52,920][mgbdt.log] \u001b[32m[epoch=53/200][train] loss=431.734628\u001b[0m\n",
      "[ 2025-02-08 15:42:53,451][mgbdt.log] \u001b[32m[epoch=54/200][train] loss=421.222532\u001b[0m\n",
      "[ 2025-02-08 15:42:54,011][mgbdt.log] \u001b[32m[epoch=55/200][train] loss=410.582743\u001b[0m\n",
      "[ 2025-02-08 15:42:54,618][mgbdt.log] \u001b[32m[epoch=56/200][train] loss=399.112308\u001b[0m\n",
      "[ 2025-02-08 15:42:55,218][mgbdt.log] \u001b[32m[epoch=57/200][train] loss=387.100313\u001b[0m\n",
      "[ 2025-02-08 15:42:55,790][mgbdt.log] \u001b[32m[epoch=58/200][train] loss=374.292728\u001b[0m\n",
      "[ 2025-02-08 15:42:56,397][mgbdt.log] \u001b[32m[epoch=59/200][train] loss=360.515429\u001b[0m\n",
      "[ 2025-02-08 15:42:56,955][mgbdt.log] \u001b[32m[epoch=60/200][train] loss=345.483983\u001b[0m\n",
      "[ 2025-02-08 15:42:57,532][mgbdt.log] \u001b[32m[epoch=61/200][train] loss=329.626828\u001b[0m\n",
      "[ 2025-02-08 15:42:58,154][mgbdt.log] \u001b[32m[epoch=62/200][train] loss=313.638201\u001b[0m\n",
      "[ 2025-02-08 15:42:58,728][mgbdt.log] \u001b[32m[epoch=63/200][train] loss=297.231772\u001b[0m\n",
      "[ 2025-02-08 15:42:59,335][mgbdt.log] \u001b[32m[epoch=64/200][train] loss=280.739997\u001b[0m\n",
      "[ 2025-02-08 15:42:59,967][mgbdt.log] \u001b[32m[epoch=65/200][train] loss=263.618806\u001b[0m\n",
      "[ 2025-02-08 15:43:00,573][mgbdt.log] \u001b[32m[epoch=66/200][train] loss=245.949794\u001b[0m\n",
      "[ 2025-02-08 15:43:01,214][mgbdt.log] \u001b[32m[epoch=67/200][train] loss=228.303395\u001b[0m\n",
      "[ 2025-02-08 15:43:01,851][mgbdt.log] \u001b[32m[epoch=68/200][train] loss=211.033047\u001b[0m\n",
      "[ 2025-02-08 15:43:02,472][mgbdt.log] \u001b[32m[epoch=69/200][train] loss=194.079670\u001b[0m\n",
      "[ 2025-02-08 15:43:03,112][mgbdt.log] \u001b[32m[epoch=70/200][train] loss=177.582837\u001b[0m\n",
      "[ 2025-02-08 15:43:03,744][mgbdt.log] \u001b[32m[epoch=71/200][train] loss=161.708295\u001b[0m\n",
      "[ 2025-02-08 15:43:04,379][mgbdt.log] \u001b[32m[epoch=72/200][train] loss=146.956168\u001b[0m\n",
      "[ 2025-02-08 15:43:05,003][mgbdt.log] \u001b[32m[epoch=73/200][train] loss=132.530750\u001b[0m\n",
      "[ 2025-02-08 15:43:05,650][mgbdt.log] \u001b[32m[epoch=74/200][train] loss=120.012130\u001b[0m\n",
      "[ 2025-02-08 15:43:06,285][mgbdt.log] \u001b[32m[epoch=75/200][train] loss=107.870233\u001b[0m\n",
      "[ 2025-02-08 15:43:06,925][mgbdt.log] \u001b[32m[epoch=76/200][train] loss=96.926191\u001b[0m\n",
      "[ 2025-02-08 15:43:07,576][mgbdt.log] \u001b[32m[epoch=77/200][train] loss=87.385385\u001b[0m\n",
      "[ 2025-02-08 15:43:08,229][mgbdt.log] \u001b[32m[epoch=78/200][train] loss=78.598290\u001b[0m\n",
      "[ 2025-02-08 15:43:08,898][mgbdt.log] \u001b[32m[epoch=79/200][train] loss=70.731405\u001b[0m\n",
      "[ 2025-02-08 15:43:09,568][mgbdt.log] \u001b[32m[epoch=80/200][train] loss=63.436758\u001b[0m\n",
      "[ 2025-02-08 15:43:10,221][mgbdt.log] \u001b[32m[epoch=81/200][train] loss=57.052754\u001b[0m\n",
      "[ 2025-02-08 15:43:10,905][mgbdt.log] \u001b[32m[epoch=82/200][train] loss=52.421510\u001b[0m\n",
      "[ 2025-02-08 15:43:11,592][mgbdt.log] \u001b[32m[epoch=83/200][train] loss=47.905760\u001b[0m\n",
      "[ 2025-02-08 15:43:12,263][mgbdt.log] \u001b[32m[epoch=84/200][train] loss=45.478820\u001b[0m\n",
      "[ 2025-02-08 15:43:12,948][mgbdt.log] \u001b[32m[epoch=85/200][train] loss=43.492950\u001b[0m\n",
      "[ 2025-02-08 15:43:13,633][mgbdt.log] \u001b[32m[epoch=86/200][train] loss=40.864402\u001b[0m\n",
      "[ 2025-02-08 15:43:14,298][mgbdt.log] \u001b[32m[epoch=87/200][train] loss=38.325579\u001b[0m\n",
      "[ 2025-02-08 15:43:15,014][mgbdt.log] \u001b[32m[epoch=88/200][train] loss=38.868627\u001b[0m\n",
      "[ 2025-02-08 15:43:15,684][mgbdt.log] \u001b[32m[epoch=89/200][train] loss=38.354502\u001b[0m\n",
      "[ 2025-02-08 15:43:16,370][mgbdt.log] \u001b[32m[epoch=90/200][train] loss=36.135927\u001b[0m\n",
      "[ 2025-02-08 15:43:17,056][mgbdt.log] \u001b[32m[epoch=91/200][train] loss=33.102809\u001b[0m\n",
      "[ 2025-02-08 15:43:17,772][mgbdt.log] \u001b[32m[epoch=92/200][train] loss=30.041579\u001b[0m\n",
      "[ 2025-02-08 15:43:18,452][mgbdt.log] \u001b[32m[epoch=93/200][train] loss=27.865765\u001b[0m\n",
      "[ 2025-02-08 15:43:19,137][mgbdt.log] \u001b[32m[epoch=94/200][train] loss=25.218125\u001b[0m\n",
      "[ 2025-02-08 15:43:19,852][mgbdt.log] \u001b[32m[epoch=95/200][train] loss=23.413234\u001b[0m\n",
      "[ 2025-02-08 15:43:20,601][mgbdt.log] \u001b[32m[epoch=96/200][train] loss=22.578841\u001b[0m\n",
      "[ 2025-02-08 15:43:21,289][mgbdt.log] \u001b[32m[epoch=97/200][train] loss=21.185989\u001b[0m\n",
      "[ 2025-02-08 15:43:22,020][mgbdt.log] \u001b[32m[epoch=98/200][train] loss=18.843230\u001b[0m\n",
      "[ 2025-02-08 15:43:22,724][mgbdt.log] \u001b[32m[epoch=99/200][train] loss=17.799923\u001b[0m\n",
      "[ 2025-02-08 15:43:23,460][mgbdt.log] \u001b[32m[epoch=100/200][train] loss=16.074487\u001b[0m\n",
      "[ 2025-02-08 15:43:24,209][mgbdt.log] \u001b[32m[epoch=101/200][train] loss=15.528399\u001b[0m\n",
      "[ 2025-02-08 15:43:24,927][mgbdt.log] \u001b[32m[epoch=102/200][train] loss=14.360050\u001b[0m\n",
      "[ 2025-02-08 15:43:25,643][mgbdt.log] \u001b[32m[epoch=103/200][train] loss=14.259446\u001b[0m\n",
      "[ 2025-02-08 15:43:26,392][mgbdt.log] \u001b[32m[epoch=104/200][train] loss=13.331013\u001b[0m\n",
      "[ 2025-02-08 15:43:27,124][mgbdt.log] \u001b[32m[epoch=105/200][train] loss=12.181279\u001b[0m\n",
      "[ 2025-02-08 15:43:27,870][mgbdt.log] \u001b[32m[epoch=106/200][train] loss=12.067418\u001b[0m\n",
      "[ 2025-02-08 15:43:28,602][mgbdt.log] \u001b[32m[epoch=107/200][train] loss=11.569846\u001b[0m\n",
      "[ 2025-02-08 15:43:29,334][mgbdt.log] \u001b[32m[epoch=108/200][train] loss=10.926127\u001b[0m\n",
      "[ 2025-02-08 15:43:30,087][mgbdt.log] \u001b[32m[epoch=109/200][train] loss=10.461097\u001b[0m\n",
      "[ 2025-02-08 15:43:30,836][mgbdt.log] \u001b[32m[epoch=110/200][train] loss=10.079544\u001b[0m\n",
      "[ 2025-02-08 15:43:31,583][mgbdt.log] \u001b[32m[epoch=111/200][train] loss=9.597434\u001b[0m\n",
      "[ 2025-02-08 15:43:32,328][mgbdt.log] \u001b[32m[epoch=112/200][train] loss=9.822619\u001b[0m\n",
      "[ 2025-02-08 15:43:33,078][mgbdt.log] \u001b[32m[epoch=113/200][train] loss=8.977066\u001b[0m\n",
      "[ 2025-02-08 15:43:33,858][mgbdt.log] \u001b[32m[epoch=114/200][train] loss=9.154220\u001b[0m\n",
      "[ 2025-02-08 15:43:34,650][mgbdt.log] \u001b[32m[epoch=115/200][train] loss=8.630692\u001b[0m\n",
      "[ 2025-02-08 15:43:35,443][mgbdt.log] \u001b[32m[epoch=116/200][train] loss=8.039319\u001b[0m\n",
      "[ 2025-02-08 15:43:36,222][mgbdt.log] \u001b[32m[epoch=117/200][train] loss=7.892725\u001b[0m\n",
      "[ 2025-02-08 15:43:37,034][mgbdt.log] \u001b[32m[epoch=118/200][train] loss=7.819713\u001b[0m\n",
      "[ 2025-02-08 15:43:37,812][mgbdt.log] \u001b[32m[epoch=119/200][train] loss=7.064146\u001b[0m\n",
      "[ 2025-02-08 15:43:38,608][mgbdt.log] \u001b[32m[epoch=120/200][train] loss=7.148530\u001b[0m\n",
      "[ 2025-02-08 15:43:39,357][mgbdt.log] \u001b[32m[epoch=121/200][train] loss=6.967719\u001b[0m\n",
      "[ 2025-02-08 15:43:40,138][mgbdt.log] \u001b[32m[epoch=122/200][train] loss=7.238567\u001b[0m\n",
      "[ 2025-02-08 15:43:40,920][mgbdt.log] \u001b[32m[epoch=123/200][train] loss=6.820676\u001b[0m\n",
      "[ 2025-02-08 15:43:41,714][mgbdt.log] \u001b[32m[epoch=124/200][train] loss=7.010034\u001b[0m\n",
      "[ 2025-02-08 15:43:42,507][mgbdt.log] \u001b[32m[epoch=125/200][train] loss=6.273919\u001b[0m\n",
      "[ 2025-02-08 15:43:43,348][mgbdt.log] \u001b[32m[epoch=126/200][train] loss=5.793434\u001b[0m\n",
      "[ 2025-02-08 15:43:44,188][mgbdt.log] \u001b[32m[epoch=127/200][train] loss=6.557424\u001b[0m\n",
      "[ 2025-02-08 15:43:44,997][mgbdt.log] \u001b[32m[epoch=128/200][train] loss=5.509413\u001b[0m\n",
      "[ 2025-02-08 15:43:45,789][mgbdt.log] \u001b[32m[epoch=129/200][train] loss=5.765104\u001b[0m\n",
      "[ 2025-02-08 15:43:46,585][mgbdt.log] \u001b[32m[epoch=130/200][train] loss=5.902803\u001b[0m\n",
      "[ 2025-02-08 15:43:47,412][mgbdt.log] \u001b[32m[epoch=131/200][train] loss=5.659043\u001b[0m\n",
      "[ 2025-02-08 15:43:48,209][mgbdt.log] \u001b[32m[epoch=132/200][train] loss=5.148649\u001b[0m\n",
      "[ 2025-02-08 15:43:49,004][mgbdt.log] \u001b[32m[epoch=133/200][train] loss=4.663609\u001b[0m\n",
      "[ 2025-02-08 15:43:49,831][mgbdt.log] \u001b[32m[epoch=134/200][train] loss=4.193433\u001b[0m\n",
      "[ 2025-02-08 15:43:50,777][mgbdt.log] \u001b[32m[epoch=135/200][train] loss=4.027926\u001b[0m\n",
      "[ 2025-02-08 15:43:51,707][mgbdt.log] \u001b[32m[epoch=136/200][train] loss=3.901330\u001b[0m\n",
      "[ 2025-02-08 15:43:52,636][mgbdt.log] \u001b[32m[epoch=137/200][train] loss=3.831225\u001b[0m\n",
      "[ 2025-02-08 15:43:53,595][mgbdt.log] \u001b[32m[epoch=138/200][train] loss=3.257925\u001b[0m\n",
      "[ 2025-02-08 15:43:54,546][mgbdt.log] \u001b[32m[epoch=139/200][train] loss=2.796876\u001b[0m\n",
      "[ 2025-02-08 15:43:55,466][mgbdt.log] \u001b[32m[epoch=140/200][train] loss=3.104574\u001b[0m\n",
      "[ 2025-02-08 15:43:56,414][mgbdt.log] \u001b[32m[epoch=141/200][train] loss=3.240803\u001b[0m\n",
      "[ 2025-02-08 15:43:57,317][mgbdt.log] \u001b[32m[epoch=142/200][train] loss=3.235521\u001b[0m\n",
      "[ 2025-02-08 15:43:58,204][mgbdt.log] \u001b[32m[epoch=143/200][train] loss=2.977598\u001b[0m\n",
      "[ 2025-02-08 15:43:59,077][mgbdt.log] \u001b[32m[epoch=144/200][train] loss=3.280705\u001b[0m\n",
      "[ 2025-02-08 15:43:59,985][mgbdt.log] \u001b[32m[epoch=145/200][train] loss=3.521715\u001b[0m\n",
      "[ 2025-02-08 15:44:00,911][mgbdt.log] \u001b[32m[epoch=146/200][train] loss=3.427340\u001b[0m\n",
      "[ 2025-02-08 15:44:01,809][mgbdt.log] \u001b[32m[epoch=147/200][train] loss=3.521857\u001b[0m\n",
      "[ 2025-02-08 15:44:02,726][mgbdt.log] \u001b[32m[epoch=148/200][train] loss=3.070413\u001b[0m\n",
      "[ 2025-02-08 15:44:03,650][mgbdt.log] \u001b[32m[epoch=149/200][train] loss=3.052865\u001b[0m\n",
      "[ 2025-02-08 15:44:04,583][mgbdt.log] \u001b[32m[epoch=150/200][train] loss=2.964419\u001b[0m\n",
      "[ 2025-02-08 15:44:05,504][mgbdt.log] \u001b[32m[epoch=151/200][train] loss=3.075015\u001b[0m\n",
      "[ 2025-02-08 15:44:06,455][mgbdt.log] \u001b[32m[epoch=152/200][train] loss=3.077975\u001b[0m\n",
      "[ 2025-02-08 15:44:07,374][mgbdt.log] \u001b[32m[epoch=153/200][train] loss=2.840206\u001b[0m\n",
      "[ 2025-02-08 15:44:08,372][mgbdt.log] \u001b[32m[epoch=154/200][train] loss=3.018296\u001b[0m\n",
      "[ 2025-02-08 15:44:09,338][mgbdt.log] \u001b[32m[epoch=155/200][train] loss=2.951334\u001b[0m\n",
      "[ 2025-02-08 15:44:10,317][mgbdt.log] \u001b[32m[epoch=156/200][train] loss=2.629838\u001b[0m\n",
      "[ 2025-02-08 15:44:11,298][mgbdt.log] \u001b[32m[epoch=157/200][train] loss=2.521171\u001b[0m\n",
      "[ 2025-02-08 15:44:12,322][mgbdt.log] \u001b[32m[epoch=158/200][train] loss=2.249230\u001b[0m\n",
      "[ 2025-02-08 15:44:13,315][mgbdt.log] \u001b[32m[epoch=159/200][train] loss=2.174638\u001b[0m\n",
      "[ 2025-02-08 15:44:14,339][mgbdt.log] \u001b[32m[epoch=160/200][train] loss=1.957312\u001b[0m\n",
      "[ 2025-02-08 15:44:15,340][mgbdt.log] \u001b[32m[epoch=161/200][train] loss=2.222600\u001b[0m\n",
      "[ 2025-02-08 15:44:16,321][mgbdt.log] \u001b[32m[epoch=162/200][train] loss=2.280980\u001b[0m\n",
      "[ 2025-02-08 15:44:17,305][mgbdt.log] \u001b[32m[epoch=163/200][train] loss=2.326070\u001b[0m\n",
      "[ 2025-02-08 15:44:18,318][mgbdt.log] \u001b[32m[epoch=164/200][train] loss=2.065108\u001b[0m\n",
      "[ 2025-02-08 15:44:19,319][mgbdt.log] \u001b[32m[epoch=165/200][train] loss=2.256826\u001b[0m\n",
      "[ 2025-02-08 15:44:20,329][mgbdt.log] \u001b[32m[epoch=166/200][train] loss=2.087750\u001b[0m\n",
      "[ 2025-02-08 15:44:21,373][mgbdt.log] \u001b[32m[epoch=167/200][train] loss=2.055215\u001b[0m\n",
      "[ 2025-02-08 15:44:22,397][mgbdt.log] \u001b[32m[epoch=168/200][train] loss=1.929256\u001b[0m\n",
      "[ 2025-02-08 15:44:23,458][mgbdt.log] \u001b[32m[epoch=169/200][train] loss=1.808342\u001b[0m\n",
      "[ 2025-02-08 15:44:24,482][mgbdt.log] \u001b[32m[epoch=170/200][train] loss=1.917890\u001b[0m\n",
      "[ 2025-02-08 15:44:25,506][mgbdt.log] \u001b[32m[epoch=171/200][train] loss=1.978029\u001b[0m\n",
      "[ 2025-02-08 15:44:26,522][mgbdt.log] \u001b[32m[epoch=172/200][train] loss=1.958262\u001b[0m\n",
      "[ 2025-02-08 15:44:27,581][mgbdt.log] \u001b[32m[epoch=173/200][train] loss=2.141607\u001b[0m\n",
      "[ 2025-02-08 15:44:28,704][mgbdt.log] \u001b[32m[epoch=174/200][train] loss=2.346274\u001b[0m\n",
      "[ 2025-02-08 15:44:29,733][mgbdt.log] \u001b[32m[epoch=175/200][train] loss=2.378340\u001b[0m\n",
      "[ 2025-02-08 15:44:30,837][mgbdt.log] \u001b[32m[epoch=176/200][train] loss=2.373295\u001b[0m\n",
      "[ 2025-02-08 15:44:31,958][mgbdt.log] \u001b[32m[epoch=177/200][train] loss=2.468370\u001b[0m\n",
      "[ 2025-02-08 15:44:33,064][mgbdt.log] \u001b[32m[epoch=178/200][train] loss=2.377958\u001b[0m\n",
      "[ 2025-02-08 15:44:34,121][mgbdt.log] \u001b[32m[epoch=179/200][train] loss=2.261853\u001b[0m\n",
      "[ 2025-02-08 15:44:35,226][mgbdt.log] \u001b[32m[epoch=180/200][train] loss=2.299625\u001b[0m\n",
      "[ 2025-02-08 15:44:36,347][mgbdt.log] \u001b[32m[epoch=181/200][train] loss=2.370258\u001b[0m\n",
      "[ 2025-02-08 15:44:37,435][mgbdt.log] \u001b[32m[epoch=182/200][train] loss=2.452407\u001b[0m\n",
      "[ 2025-02-08 15:44:38,541][mgbdt.log] \u001b[32m[epoch=183/200][train] loss=2.362335\u001b[0m\n",
      "[ 2025-02-08 15:44:39,648][mgbdt.log] \u001b[32m[epoch=184/200][train] loss=2.310812\u001b[0m\n",
      "[ 2025-02-08 15:44:40,740][mgbdt.log] \u001b[32m[epoch=185/200][train] loss=2.635703\u001b[0m\n",
      "[ 2025-02-08 15:44:41,866][mgbdt.log] \u001b[32m[epoch=186/200][train] loss=2.606263\u001b[0m\n",
      "[ 2025-02-08 15:44:42,925][mgbdt.log] \u001b[32m[epoch=187/200][train] loss=2.664371\u001b[0m\n",
      "[ 2025-02-08 15:44:44,069][mgbdt.log] \u001b[32m[epoch=188/200][train] loss=2.700013\u001b[0m\n",
      "[ 2025-02-08 15:44:45,202][mgbdt.log] \u001b[32m[epoch=189/200][train] loss=2.825352\u001b[0m\n",
      "[ 2025-02-08 15:44:46,353][mgbdt.log] \u001b[32m[epoch=190/200][train] loss=2.859765\u001b[0m\n",
      "[ 2025-02-08 15:44:47,554][mgbdt.log] \u001b[32m[epoch=191/200][train] loss=3.120212\u001b[0m\n",
      "[ 2025-02-08 15:44:48,704][mgbdt.log] \u001b[32m[epoch=192/200][train] loss=3.245326\u001b[0m\n",
      "[ 2025-02-08 15:44:49,889][mgbdt.log] \u001b[32m[epoch=193/200][train] loss=3.480743\u001b[0m\n",
      "[ 2025-02-08 15:44:51,101][mgbdt.log] \u001b[32m[epoch=194/200][train] loss=3.372455\u001b[0m\n",
      "[ 2025-02-08 15:44:52,339][mgbdt.log] \u001b[32m[epoch=195/200][train] loss=3.484996\u001b[0m\n",
      "[ 2025-02-08 15:44:53,581][mgbdt.log] \u001b[32m[epoch=196/200][train] loss=4.207479\u001b[0m\n",
      "[ 2025-02-08 15:44:54,870][mgbdt.log] \u001b[32m[epoch=197/200][train] loss=4.010534\u001b[0m\n",
      "[ 2025-02-08 15:44:56,143][mgbdt.log] \u001b[32m[epoch=198/200][train] loss=4.254591\u001b[0m\n",
      "[ 2025-02-08 15:44:57,448][mgbdt.log] \u001b[32m[epoch=199/200][train] loss=4.137853\u001b[0m\n",
      "[ 2025-02-08 15:44:58,772][mgbdt.log] \u001b[32m[epoch=200/200][train] loss=4.896346\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "net_linear = MGBDT(loss=None, target_lr=1, epsilon=0.1, verbose=False) \n",
    "\n",
    "# Add target-propogation layers: F, G represent the forward and inverse mapping layers, respectively\n",
    "net_linear.add_layer(\"tp_layer\",\n",
    "                     F=MultiXGBModel(input_size=X_train.shape[1], output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                     G=None)\n",
    "net_linear.add_layer(\"tp_layer\",\n",
    "                     F=MultiXGBModel(input_size=5, output_size=3, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                     G=MultiXGBModel(input_size=3, output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round))\n",
    "net_linear.add_layer(\"bp_layer\",\n",
    "                     F=LinearModel(input_size=3, output_size=1, learning_rate=0.01, loss=loss))\n",
    "\n",
    "\n",
    "# init the forward mapping\n",
    "net_linear.init(X_train, n_rounds=10)\n",
    "\n",
    "net_linear.fit(X_train, y_train.reshape(-1,1), n_epochs=n_epochs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab9690-38ae-4282-b784-6c67d53f06db",
   "metadata": {},
   "source": [
    "## mGBDT XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39719b30-1cfb-4caa-91b1-771f8de06e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[ 2025-02-08 15:44:59,079][mgbdt.log] \u001b[32m[epoch=0/200][train] loss=22.386313\u001b[0m\n",
      "[ 2025-02-08 15:44:59,699][mgbdt.log] \u001b[32m[epoch=1/200][train] loss=22.244833\u001b[0m\n",
      "[ 2025-02-08 15:45:00,321][mgbdt.log] \u001b[32m[epoch=2/200][train] loss=22.115594\u001b[0m\n",
      "[ 2025-02-08 15:45:01,082][mgbdt.log] \u001b[32m[epoch=3/200][train] loss=21.971818\u001b[0m\n",
      "[ 2025-02-08 15:45:01,703][mgbdt.log] \u001b[32m[epoch=4/200][train] loss=21.832660\u001b[0m\n",
      "[ 2025-02-08 15:45:02,324][mgbdt.log] \u001b[32m[epoch=5/200][train] loss=21.691416\u001b[0m\n",
      "[ 2025-02-08 15:45:02,944][mgbdt.log] \u001b[32m[epoch=6/200][train] loss=21.550115\u001b[0m\n",
      "[ 2025-02-08 15:45:03,568][mgbdt.log] \u001b[32m[epoch=7/200][train] loss=21.408707\u001b[0m\n",
      "[ 2025-02-08 15:45:04,187][mgbdt.log] \u001b[32m[epoch=8/200][train] loss=21.266464\u001b[0m\n",
      "[ 2025-02-08 15:45:04,821][mgbdt.log] \u001b[32m[epoch=9/200][train] loss=21.123350\u001b[0m\n",
      "[ 2025-02-08 15:45:05,476][mgbdt.log] \u001b[32m[epoch=10/200][train] loss=20.981255\u001b[0m\n",
      "[ 2025-02-08 15:45:06,129][mgbdt.log] \u001b[32m[epoch=11/200][train] loss=20.841565\u001b[0m\n",
      "[ 2025-02-08 15:45:06,783][mgbdt.log] \u001b[32m[epoch=12/200][train] loss=20.700675\u001b[0m\n",
      "[ 2025-02-08 15:45:07,431][mgbdt.log] \u001b[32m[epoch=13/200][train] loss=20.559717\u001b[0m\n",
      "[ 2025-02-08 15:45:08,085][mgbdt.log] \u001b[32m[epoch=14/200][train] loss=20.419130\u001b[0m\n",
      "[ 2025-02-08 15:45:08,750][mgbdt.log] \u001b[32m[epoch=15/200][train] loss=20.278527\u001b[0m\n",
      "[ 2025-02-08 15:45:09,407][mgbdt.log] \u001b[32m[epoch=16/200][train] loss=20.142458\u001b[0m\n",
      "[ 2025-02-08 15:45:10,062][mgbdt.log] \u001b[32m[epoch=17/200][train] loss=20.025487\u001b[0m\n",
      "[ 2025-02-08 15:45:10,715][mgbdt.log] \u001b[32m[epoch=18/200][train] loss=19.891042\u001b[0m\n",
      "[ 2025-02-08 15:45:11,368][mgbdt.log] \u001b[32m[epoch=19/200][train] loss=19.750360\u001b[0m\n",
      "[ 2025-02-08 15:45:12,022][mgbdt.log] \u001b[32m[epoch=20/200][train] loss=19.571158\u001b[0m\n",
      "[ 2025-02-08 15:45:12,677][mgbdt.log] \u001b[32m[epoch=21/200][train] loss=19.432934\u001b[0m\n",
      "[ 2025-02-08 15:45:13,343][mgbdt.log] \u001b[32m[epoch=22/200][train] loss=19.290672\u001b[0m\n",
      "[ 2025-02-08 15:45:14,030][mgbdt.log] \u001b[32m[epoch=23/200][train] loss=19.149232\u001b[0m\n",
      "[ 2025-02-08 15:45:14,717][mgbdt.log] \u001b[32m[epoch=24/200][train] loss=18.990334\u001b[0m\n",
      "[ 2025-02-08 15:45:15,400][mgbdt.log] \u001b[32m[epoch=25/200][train] loss=18.766174\u001b[0m\n",
      "[ 2025-02-08 15:45:16,088][mgbdt.log] \u001b[32m[epoch=26/200][train] loss=18.623850\u001b[0m\n",
      "[ 2025-02-08 15:45:16,785][mgbdt.log] \u001b[32m[epoch=27/200][train] loss=18.653268\u001b[0m\n",
      "[ 2025-02-08 15:45:17,486][mgbdt.log] \u001b[32m[epoch=28/200][train] loss=18.510942\u001b[0m\n",
      "[ 2025-02-08 15:45:18,186][mgbdt.log] \u001b[32m[epoch=29/200][train] loss=18.182810\u001b[0m\n",
      "[ 2025-02-08 15:45:18,879][mgbdt.log] \u001b[32m[epoch=30/200][train] loss=18.042671\u001b[0m\n",
      "[ 2025-02-08 15:45:19,579][mgbdt.log] \u001b[32m[epoch=31/200][train] loss=17.902737\u001b[0m\n",
      "[ 2025-02-08 15:45:20,277][mgbdt.log] \u001b[32m[epoch=32/200][train] loss=17.761128\u001b[0m\n",
      "[ 2025-02-08 15:45:21,001][mgbdt.log] \u001b[32m[epoch=33/200][train] loss=17.621163\u001b[0m\n",
      "[ 2025-02-08 15:45:21,717][mgbdt.log] \u001b[32m[epoch=34/200][train] loss=17.494816\u001b[0m\n",
      "[ 2025-02-08 15:45:22,429][mgbdt.log] \u001b[32m[epoch=35/200][train] loss=17.480896\u001b[0m\n",
      "[ 2025-02-08 15:45:23,146][mgbdt.log] \u001b[32m[epoch=36/200][train] loss=17.359496\u001b[0m\n",
      "[ 2025-02-08 15:45:23,844][mgbdt.log] \u001b[32m[epoch=37/200][train] loss=17.220812\u001b[0m\n",
      "[ 2025-02-08 15:45:24,552][mgbdt.log] \u001b[32m[epoch=38/200][train] loss=17.091419\u001b[0m\n",
      "[ 2025-02-08 15:45:25,284][mgbdt.log] \u001b[32m[epoch=39/200][train] loss=16.957051\u001b[0m\n",
      "[ 2025-02-08 15:45:25,996][mgbdt.log] \u001b[32m[epoch=40/200][train] loss=16.817930\u001b[0m\n",
      "[ 2025-02-08 15:45:26,712][mgbdt.log] \u001b[32m[epoch=41/200][train] loss=16.683059\u001b[0m\n",
      "[ 2025-02-08 15:45:27,443][mgbdt.log] \u001b[32m[epoch=42/200][train] loss=16.541847\u001b[0m\n",
      "[ 2025-02-08 15:45:28,219][mgbdt.log] \u001b[32m[epoch=43/200][train] loss=16.403833\u001b[0m\n",
      "[ 2025-02-08 15:45:28,976][mgbdt.log] \u001b[32m[epoch=44/200][train] loss=16.261873\u001b[0m\n",
      "[ 2025-02-08 15:45:29,743][mgbdt.log] \u001b[32m[epoch=45/200][train] loss=16.125552\u001b[0m\n",
      "[ 2025-02-08 15:45:30,506][mgbdt.log] \u001b[32m[epoch=46/200][train] loss=15.985997\u001b[0m\n",
      "[ 2025-02-08 15:45:31,300][mgbdt.log] \u001b[32m[epoch=47/200][train] loss=15.860063\u001b[0m\n",
      "[ 2025-02-08 15:45:32,092][mgbdt.log] \u001b[32m[epoch=48/200][train] loss=15.714488\u001b[0m\n",
      "[ 2025-02-08 15:45:32,902][mgbdt.log] \u001b[32m[epoch=49/200][train] loss=15.572463\u001b[0m\n",
      "[ 2025-02-08 15:45:33,726][mgbdt.log] \u001b[32m[epoch=50/200][train] loss=15.448717\u001b[0m\n",
      "[ 2025-02-08 15:45:34,514][mgbdt.log] \u001b[32m[epoch=51/200][train] loss=15.320770\u001b[0m\n",
      "[ 2025-02-08 15:45:35,336][mgbdt.log] \u001b[32m[epoch=52/200][train] loss=15.192237\u001b[0m\n",
      "[ 2025-02-08 15:45:36,114][mgbdt.log] \u001b[32m[epoch=53/200][train] loss=15.063332\u001b[0m\n",
      "[ 2025-02-08 15:45:36,935][mgbdt.log] \u001b[32m[epoch=54/200][train] loss=14.920287\u001b[0m\n",
      "[ 2025-02-08 15:45:37,753][mgbdt.log] \u001b[32m[epoch=55/200][train] loss=14.792911\u001b[0m\n",
      "[ 2025-02-08 15:45:38,565][mgbdt.log] \u001b[32m[epoch=56/200][train] loss=14.658553\u001b[0m\n",
      "[ 2025-02-08 15:45:39,392][mgbdt.log] \u001b[32m[epoch=57/200][train] loss=14.516773\u001b[0m\n",
      "[ 2025-02-08 15:45:40,186][mgbdt.log] \u001b[32m[epoch=58/200][train] loss=14.393218\u001b[0m\n",
      "[ 2025-02-08 15:45:40,981][mgbdt.log] \u001b[32m[epoch=59/200][train] loss=14.255772\u001b[0m\n",
      "[ 2025-02-08 15:45:41,804][mgbdt.log] \u001b[32m[epoch=60/200][train] loss=14.126993\u001b[0m\n",
      "[ 2025-02-08 15:45:42,661][mgbdt.log] \u001b[32m[epoch=61/200][train] loss=14.002153\u001b[0m\n",
      "[ 2025-02-08 15:45:43,543][mgbdt.log] \u001b[32m[epoch=62/200][train] loss=13.887032\u001b[0m\n",
      "[ 2025-02-08 15:45:44,336][mgbdt.log] \u001b[32m[epoch=63/200][train] loss=13.753517\u001b[0m\n",
      "[ 2025-02-08 15:45:45,174][mgbdt.log] \u001b[32m[epoch=64/200][train] loss=13.618721\u001b[0m\n",
      "[ 2025-02-08 15:45:46,032][mgbdt.log] \u001b[32m[epoch=65/200][train] loss=13.497484\u001b[0m\n",
      "[ 2025-02-08 15:45:46,872][mgbdt.log] \u001b[32m[epoch=66/200][train] loss=13.371483\u001b[0m\n",
      "[ 2025-02-08 15:45:47,760][mgbdt.log] \u001b[32m[epoch=67/200][train] loss=13.239633\u001b[0m\n",
      "[ 2025-02-08 15:45:48,646][mgbdt.log] \u001b[32m[epoch=68/200][train] loss=13.137515\u001b[0m\n",
      "[ 2025-02-08 15:45:49,528][mgbdt.log] \u001b[32m[epoch=69/200][train] loss=13.046351\u001b[0m\n",
      "[ 2025-02-08 15:45:50,431][mgbdt.log] \u001b[32m[epoch=70/200][train] loss=12.916293\u001b[0m\n",
      "[ 2025-02-08 15:45:51,334][mgbdt.log] \u001b[32m[epoch=71/200][train] loss=12.794232\u001b[0m\n",
      "[ 2025-02-08 15:45:52,229][mgbdt.log] \u001b[32m[epoch=72/200][train] loss=12.711157\u001b[0m\n",
      "[ 2025-02-08 15:45:53,131][mgbdt.log] \u001b[32m[epoch=73/200][train] loss=12.641458\u001b[0m\n",
      "[ 2025-02-08 15:45:54,018][mgbdt.log] \u001b[32m[epoch=74/200][train] loss=12.627063\u001b[0m\n",
      "[ 2025-02-08 15:45:54,933][mgbdt.log] \u001b[32m[epoch=75/200][train] loss=12.463456\u001b[0m\n",
      "[ 2025-02-08 15:45:55,822][mgbdt.log] \u001b[32m[epoch=76/200][train] loss=12.326324\u001b[0m\n",
      "[ 2025-02-08 15:45:56,738][mgbdt.log] \u001b[32m[epoch=77/200][train] loss=12.184788\u001b[0m\n",
      "[ 2025-02-08 15:45:57,640][mgbdt.log] \u001b[32m[epoch=78/200][train] loss=12.071223\u001b[0m\n",
      "[ 2025-02-08 15:45:58,572][mgbdt.log] \u001b[32m[epoch=79/200][train] loss=11.982563\u001b[0m\n",
      "[ 2025-02-08 15:45:59,491][mgbdt.log] \u001b[32m[epoch=80/200][train] loss=11.896054\u001b[0m\n",
      "[ 2025-02-08 15:46:00,388][mgbdt.log] \u001b[32m[epoch=81/200][train] loss=11.790116\u001b[0m\n",
      "[ 2025-02-08 15:46:01,332][mgbdt.log] \u001b[32m[epoch=82/200][train] loss=11.735536\u001b[0m\n",
      "[ 2025-02-08 15:46:02,250][mgbdt.log] \u001b[32m[epoch=83/200][train] loss=11.644173\u001b[0m\n",
      "[ 2025-02-08 15:46:03,178][mgbdt.log] \u001b[32m[epoch=84/200][train] loss=11.540480\u001b[0m\n",
      "[ 2025-02-08 15:46:04,121][mgbdt.log] \u001b[32m[epoch=85/200][train] loss=11.430528\u001b[0m\n",
      "[ 2025-02-08 15:46:05,069][mgbdt.log] \u001b[32m[epoch=86/200][train] loss=11.290666\u001b[0m\n",
      "[ 2025-02-08 15:46:06,015][mgbdt.log] \u001b[32m[epoch=87/200][train] loss=11.167573\u001b[0m\n",
      "[ 2025-02-08 15:46:06,952][mgbdt.log] \u001b[32m[epoch=88/200][train] loss=11.075594\u001b[0m\n",
      "[ 2025-02-08 15:46:07,912][mgbdt.log] \u001b[32m[epoch=89/200][train] loss=10.988423\u001b[0m\n",
      "[ 2025-02-08 15:46:08,865][mgbdt.log] \u001b[32m[epoch=90/200][train] loss=10.893316\u001b[0m\n",
      "[ 2025-02-08 15:46:09,840][mgbdt.log] \u001b[32m[epoch=91/200][train] loss=10.825958\u001b[0m\n",
      "[ 2025-02-08 15:46:10,786][mgbdt.log] \u001b[32m[epoch=92/200][train] loss=10.838774\u001b[0m\n",
      "[ 2025-02-08 15:46:11,762][mgbdt.log] \u001b[32m[epoch=93/200][train] loss=10.727307\u001b[0m\n",
      "[ 2025-02-08 15:46:12,726][mgbdt.log] \u001b[32m[epoch=94/200][train] loss=10.718761\u001b[0m\n",
      "[ 2025-02-08 15:46:13,702][mgbdt.log] \u001b[32m[epoch=95/200][train] loss=10.708856\u001b[0m\n",
      "[ 2025-02-08 15:46:14,680][mgbdt.log] \u001b[32m[epoch=96/200][train] loss=10.748463\u001b[0m\n",
      "[ 2025-02-08 15:46:15,659][mgbdt.log] \u001b[32m[epoch=97/200][train] loss=10.605338\u001b[0m\n",
      "[ 2025-02-08 15:46:16,624][mgbdt.log] \u001b[32m[epoch=98/200][train] loss=10.490773\u001b[0m\n",
      "[ 2025-02-08 15:46:17,619][mgbdt.log] \u001b[32m[epoch=99/200][train] loss=10.365527\u001b[0m\n",
      "[ 2025-02-08 15:46:18,625][mgbdt.log] \u001b[32m[epoch=100/200][train] loss=10.261726\u001b[0m\n",
      "[ 2025-02-08 15:46:19,629][mgbdt.log] \u001b[32m[epoch=101/200][train] loss=10.362604\u001b[0m\n",
      "[ 2025-02-08 15:46:20,635][mgbdt.log] \u001b[32m[epoch=102/200][train] loss=10.112316\u001b[0m\n",
      "[ 2025-02-08 15:46:21,662][mgbdt.log] \u001b[32m[epoch=103/200][train] loss=9.792754\u001b[0m\n",
      "[ 2025-02-08 15:46:22,688][mgbdt.log] \u001b[32m[epoch=104/200][train] loss=9.637246\u001b[0m\n",
      "[ 2025-02-08 15:46:23,695][mgbdt.log] \u001b[32m[epoch=105/200][train] loss=9.630479\u001b[0m\n",
      "[ 2025-02-08 15:46:24,719][mgbdt.log] \u001b[32m[epoch=106/200][train] loss=9.575370\u001b[0m\n",
      "[ 2025-02-08 15:46:25,726][mgbdt.log] \u001b[32m[epoch=107/200][train] loss=9.416548\u001b[0m\n",
      "[ 2025-02-08 15:46:26,750][mgbdt.log] \u001b[32m[epoch=108/200][train] loss=9.303061\u001b[0m\n",
      "[ 2025-02-08 15:46:27,759][mgbdt.log] \u001b[32m[epoch=109/200][train] loss=9.210056\u001b[0m\n",
      "[ 2025-02-08 15:46:28,750][mgbdt.log] \u001b[32m[epoch=110/200][train] loss=9.171032\u001b[0m\n",
      "[ 2025-02-08 15:46:29,787][mgbdt.log] \u001b[32m[epoch=111/200][train] loss=9.137630\u001b[0m\n",
      "[ 2025-02-08 15:46:30,834][mgbdt.log] \u001b[32m[epoch=112/200][train] loss=9.032542\u001b[0m\n",
      "[ 2025-02-08 15:46:31,875][mgbdt.log] \u001b[32m[epoch=113/200][train] loss=8.987084\u001b[0m\n",
      "[ 2025-02-08 15:46:32,948][mgbdt.log] \u001b[32m[epoch=114/200][train] loss=8.927708\u001b[0m\n",
      "[ 2025-02-08 15:46:34,003][mgbdt.log] \u001b[32m[epoch=115/200][train] loss=8.924297\u001b[0m\n",
      "[ 2025-02-08 15:46:35,054][mgbdt.log] \u001b[32m[epoch=116/200][train] loss=8.908703\u001b[0m\n",
      "[ 2025-02-08 15:46:36,129][mgbdt.log] \u001b[32m[epoch=117/200][train] loss=8.873494\u001b[0m\n",
      "[ 2025-02-08 15:46:37,187][mgbdt.log] \u001b[32m[epoch=118/200][train] loss=8.856735\u001b[0m\n",
      "[ 2025-02-08 15:46:38,236][mgbdt.log] \u001b[32m[epoch=119/200][train] loss=8.807727\u001b[0m\n",
      "[ 2025-02-08 15:46:39,316][mgbdt.log] \u001b[32m[epoch=120/200][train] loss=8.814148\u001b[0m\n",
      "[ 2025-02-08 15:46:40,415][mgbdt.log] \u001b[32m[epoch=121/200][train] loss=8.823554\u001b[0m\n",
      "[ 2025-02-08 15:46:41,518][mgbdt.log] \u001b[32m[epoch=122/200][train] loss=8.833145\u001b[0m\n",
      "[ 2025-02-08 15:46:42,676][mgbdt.log] \u001b[32m[epoch=123/200][train] loss=8.863369\u001b[0m\n",
      "[ 2025-02-08 15:46:43,950][mgbdt.log] \u001b[32m[epoch=124/200][train] loss=8.777689\u001b[0m\n",
      "[ 2025-02-08 15:46:45,215][mgbdt.log] \u001b[32m[epoch=125/200][train] loss=8.714922\u001b[0m\n",
      "[ 2025-02-08 15:46:46,535][mgbdt.log] \u001b[32m[epoch=126/200][train] loss=8.663695\u001b[0m\n",
      "[ 2025-02-08 15:46:47,837][mgbdt.log] \u001b[32m[epoch=127/200][train] loss=8.589647\u001b[0m\n",
      "[ 2025-02-08 15:46:49,209][mgbdt.log] \u001b[32m[epoch=128/200][train] loss=8.678093\u001b[0m\n",
      "[ 2025-02-08 15:46:50,546][mgbdt.log] \u001b[32m[epoch=129/200][train] loss=8.593462\u001b[0m\n",
      "[ 2025-02-08 15:46:51,838][mgbdt.log] \u001b[32m[epoch=130/200][train] loss=8.671023\u001b[0m\n",
      "[ 2025-02-08 15:46:53,163][mgbdt.log] \u001b[32m[epoch=131/200][train] loss=8.596090\u001b[0m\n",
      "[ 2025-02-08 15:46:54,528][mgbdt.log] \u001b[32m[epoch=132/200][train] loss=8.493776\u001b[0m\n",
      "[ 2025-02-08 15:46:55,854][mgbdt.log] \u001b[32m[epoch=133/200][train] loss=8.417275\u001b[0m\n",
      "[ 2025-02-08 15:46:57,208][mgbdt.log] \u001b[32m[epoch=134/200][train] loss=8.370887\u001b[0m\n",
      "[ 2025-02-08 15:46:58,579][mgbdt.log] \u001b[32m[epoch=135/200][train] loss=8.316870\u001b[0m\n",
      "[ 2025-02-08 15:47:00,115][mgbdt.log] \u001b[32m[epoch=136/200][train] loss=8.228373\u001b[0m\n",
      "[ 2025-02-08 15:47:01,487][mgbdt.log] \u001b[32m[epoch=137/200][train] loss=8.266472\u001b[0m\n",
      "[ 2025-02-08 15:47:02,867][mgbdt.log] \u001b[32m[epoch=138/200][train] loss=8.243290\u001b[0m\n",
      "[ 2025-02-08 15:47:04,266][mgbdt.log] \u001b[32m[epoch=139/200][train] loss=8.318485\u001b[0m\n",
      "[ 2025-02-08 15:47:05,651][mgbdt.log] \u001b[32m[epoch=140/200][train] loss=8.276495\u001b[0m\n",
      "[ 2025-02-08 15:47:07,017][mgbdt.log] \u001b[32m[epoch=141/200][train] loss=8.241490\u001b[0m\n",
      "[ 2025-02-08 15:47:08,419][mgbdt.log] \u001b[32m[epoch=142/200][train] loss=8.121446\u001b[0m\n",
      "[ 2025-02-08 15:47:09,822][mgbdt.log] \u001b[32m[epoch=143/200][train] loss=8.020313\u001b[0m\n",
      "[ 2025-02-08 15:47:11,222][mgbdt.log] \u001b[32m[epoch=144/200][train] loss=7.984594\u001b[0m\n",
      "[ 2025-02-08 15:47:12,654][mgbdt.log] \u001b[32m[epoch=145/200][train] loss=7.852534\u001b[0m\n",
      "[ 2025-02-08 15:47:14,074][mgbdt.log] \u001b[32m[epoch=146/200][train] loss=7.799201\u001b[0m\n",
      "[ 2025-02-08 15:47:15,566][mgbdt.log] \u001b[32m[epoch=147/200][train] loss=7.758535\u001b[0m\n",
      "[ 2025-02-08 15:47:16,976][mgbdt.log] \u001b[32m[epoch=148/200][train] loss=7.671199\u001b[0m\n",
      "[ 2025-02-08 15:47:18,397][mgbdt.log] \u001b[32m[epoch=149/200][train] loss=7.599030\u001b[0m\n",
      "[ 2025-02-08 15:47:19,814][mgbdt.log] \u001b[32m[epoch=150/200][train] loss=7.487844\u001b[0m\n",
      "[ 2025-02-08 15:47:21,274][mgbdt.log] \u001b[32m[epoch=151/200][train] loss=7.526019\u001b[0m\n",
      "[ 2025-02-08 15:47:22,729][mgbdt.log] \u001b[32m[epoch=152/200][train] loss=7.456808\u001b[0m\n",
      "[ 2025-02-08 15:47:24,210][mgbdt.log] \u001b[32m[epoch=153/200][train] loss=7.589295\u001b[0m\n",
      "[ 2025-02-08 15:47:25,691][mgbdt.log] \u001b[32m[epoch=154/200][train] loss=7.532414\u001b[0m\n",
      "[ 2025-02-08 15:47:27,212][mgbdt.log] \u001b[32m[epoch=155/200][train] loss=7.394506\u001b[0m\n",
      "[ 2025-02-08 15:47:28,708][mgbdt.log] \u001b[32m[epoch=156/200][train] loss=7.315030\u001b[0m\n",
      "[ 2025-02-08 15:47:30,181][mgbdt.log] \u001b[32m[epoch=157/200][train] loss=7.210040\u001b[0m\n",
      "[ 2025-02-08 15:47:31,706][mgbdt.log] \u001b[32m[epoch=158/200][train] loss=7.066308\u001b[0m\n",
      "[ 2025-02-08 15:47:33,211][mgbdt.log] \u001b[32m[epoch=159/200][train] loss=7.040883\u001b[0m\n",
      "[ 2025-02-08 15:47:34,711][mgbdt.log] \u001b[32m[epoch=160/200][train] loss=6.978171\u001b[0m\n",
      "[ 2025-02-08 15:47:36,220][mgbdt.log] \u001b[32m[epoch=161/200][train] loss=6.997793\u001b[0m\n",
      "[ 2025-02-08 15:47:37,777][mgbdt.log] \u001b[32m[epoch=162/200][train] loss=7.083481\u001b[0m\n",
      "[ 2025-02-08 15:47:39,300][mgbdt.log] \u001b[32m[epoch=163/200][train] loss=7.036178\u001b[0m\n",
      "[ 2025-02-08 15:47:40,872][mgbdt.log] \u001b[32m[epoch=164/200][train] loss=7.018704\u001b[0m\n",
      "[ 2025-02-08 15:47:42,415][mgbdt.log] \u001b[32m[epoch=165/200][train] loss=7.023131\u001b[0m\n",
      "[ 2025-02-08 15:47:44,003][mgbdt.log] \u001b[32m[epoch=166/200][train] loss=7.028323\u001b[0m\n",
      "[ 2025-02-08 15:47:45,560][mgbdt.log] \u001b[32m[epoch=167/200][train] loss=7.030325\u001b[0m\n",
      "[ 2025-02-08 15:47:47,174][mgbdt.log] \u001b[32m[epoch=168/200][train] loss=7.064919\u001b[0m\n",
      "[ 2025-02-08 15:47:48,763][mgbdt.log] \u001b[32m[epoch=169/200][train] loss=7.049660\u001b[0m\n",
      "[ 2025-02-08 15:47:50,396][mgbdt.log] \u001b[32m[epoch=170/200][train] loss=7.063492\u001b[0m\n",
      "[ 2025-02-08 15:47:51,999][mgbdt.log] \u001b[32m[epoch=171/200][train] loss=7.042651\u001b[0m\n",
      "[ 2025-02-08 15:47:53,653][mgbdt.log] \u001b[32m[epoch=172/200][train] loss=7.032747\u001b[0m\n",
      "[ 2025-02-08 15:47:55,268][mgbdt.log] \u001b[32m[epoch=173/200][train] loss=7.055045\u001b[0m\n",
      "[ 2025-02-08 15:47:56,885][mgbdt.log] \u001b[32m[epoch=174/200][train] loss=7.015406\u001b[0m\n",
      "[ 2025-02-08 15:47:58,491][mgbdt.log] \u001b[32m[epoch=175/200][train] loss=7.035708\u001b[0m\n",
      "[ 2025-02-08 15:48:00,144][mgbdt.log] \u001b[32m[epoch=176/200][train] loss=7.045937\u001b[0m\n",
      "[ 2025-02-08 15:48:01,806][mgbdt.log] \u001b[32m[epoch=177/200][train] loss=7.069194\u001b[0m\n",
      "[ 2025-02-08 15:48:03,498][mgbdt.log] \u001b[32m[epoch=178/200][train] loss=7.085644\u001b[0m\n",
      "[ 2025-02-08 15:48:05,182][mgbdt.log] \u001b[32m[epoch=179/200][train] loss=7.128593\u001b[0m\n",
      "[ 2025-02-08 15:48:06,868][mgbdt.log] \u001b[32m[epoch=180/200][train] loss=7.116461\u001b[0m\n",
      "[ 2025-02-08 15:48:08,567][mgbdt.log] \u001b[32m[epoch=181/200][train] loss=7.065723\u001b[0m\n",
      "[ 2025-02-08 15:48:10,236][mgbdt.log] \u001b[32m[epoch=182/200][train] loss=7.083756\u001b[0m\n",
      "[ 2025-02-08 15:48:11,922][mgbdt.log] \u001b[32m[epoch=183/200][train] loss=6.982204\u001b[0m\n",
      "[ 2025-02-08 15:48:13,642][mgbdt.log] \u001b[32m[epoch=184/200][train] loss=6.851924\u001b[0m\n",
      "[ 2025-02-08 15:48:15,350][mgbdt.log] \u001b[32m[epoch=185/200][train] loss=6.781333\u001b[0m\n",
      "[ 2025-02-08 15:48:17,028][mgbdt.log] \u001b[32m[epoch=186/200][train] loss=6.821730\u001b[0m\n",
      "[ 2025-02-08 15:48:18,710][mgbdt.log] \u001b[32m[epoch=187/200][train] loss=6.844384\u001b[0m\n",
      "[ 2025-02-08 15:48:20,421][mgbdt.log] \u001b[32m[epoch=188/200][train] loss=6.874521\u001b[0m\n",
      "[ 2025-02-08 15:48:22,146][mgbdt.log] \u001b[32m[epoch=189/200][train] loss=7.116458\u001b[0m\n",
      "[ 2025-02-08 15:48:23,873][mgbdt.log] \u001b[32m[epoch=190/200][train] loss=7.073210\u001b[0m\n",
      "[ 2025-02-08 15:48:25,615][mgbdt.log] \u001b[32m[epoch=191/200][train] loss=7.028742\u001b[0m\n",
      "[ 2025-02-08 15:48:27,336][mgbdt.log] \u001b[32m[epoch=192/200][train] loss=6.926418\u001b[0m\n",
      "[ 2025-02-08 15:48:29,083][mgbdt.log] \u001b[32m[epoch=193/200][train] loss=6.800723\u001b[0m\n",
      "[ 2025-02-08 15:48:30,848][mgbdt.log] \u001b[32m[epoch=194/200][train] loss=6.725124\u001b[0m\n",
      "[ 2025-02-08 15:48:32,651][mgbdt.log] \u001b[32m[epoch=195/200][train] loss=6.632469\u001b[0m\n",
      "[ 2025-02-08 15:48:34,458][mgbdt.log] \u001b[32m[epoch=196/200][train] loss=6.403379\u001b[0m\n",
      "[ 2025-02-08 15:48:36,260][mgbdt.log] \u001b[32m[epoch=197/200][train] loss=6.241201\u001b[0m\n",
      "[ 2025-02-08 15:48:38,046][mgbdt.log] \u001b[32m[epoch=198/200][train] loss=6.060110\u001b[0m\n",
      "[ 2025-02-08 15:48:39,872][mgbdt.log] \u001b[32m[epoch=199/200][train] loss=5.952710\u001b[0m\n",
      "[ 2025-02-08 15:48:41,734][mgbdt.log] \u001b[32m[epoch=200/200][train] loss=5.904740\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "net_xgboost = MGBDT(loss=loss, target_lr=1, epsilon=0.1, verbose=False) \n",
    "\n",
    "# Add target-propogation layers: F, G represent the forward and inverse mapping layers, respectively\n",
    "net_xgboost.add_layer(\"tp_layer\",\n",
    "                      F=MultiXGBModel(input_size=X_train.shape[1], output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                      G=None)\n",
    "net_xgboost.add_layer(\"tp_layer\",\n",
    "                      F=MultiXGBModel(input_size=5, output_size=3, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                      G=MultiXGBModel(input_size=3, output_size=5, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round))\n",
    "net_xgboost.add_layer(\"tp_layer\",\n",
    "                      F=MultiXGBModel(input_size=3, output_size=1, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round),\n",
    "                      G=MultiXGBModel(input_size=1, output_size=3, learning_rate=learning_rate, max_depth=max_depth, num_boost_round=num_boost_round))\n",
    "\n",
    "\n",
    "# init the forward mapping\n",
    "net_xgboost.init(X_train, n_rounds=10)\n",
    "\n",
    "net_xgboost.fit(X_train, y_train.reshape(-1,1), n_epochs=n_epochs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a53254-d84a-4a79-bac7-e07db2fe0be4",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d63cd9-e177-4366-a2f3-ab5aefee0a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\mdgbt\\lib\\site-packages\\torch\\nn\\modules\\loss.py:128: UserWarning: Using a target size (torch.Size([354])) that is different to the input size (torch.Size([354, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "linear_model = LinearModel(input_size=13, output_size=1, loss=loss)\n",
    "for _ in range(n_epochs):\n",
    "    linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbd3d3-ff95-48aa-9e12-fdd311f58a13",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ad1898-e881-4eda-8819-3e0c4b2e0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"eta\": learning_rate,                   \n",
    "          \"max_depth\": max_depth\n",
    "         }\n",
    "\n",
    "xgb_model = xgb.train(params,\n",
    "                      dtrain,\n",
    "                      num_boost_round=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3badc93-4d9d-4fa4-8a32-e76849ffd53d",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61906a88-81d3-47c5-b1d5-efaffb73d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mgbdt_linear = pd.DataFrame.from_dict({\"model\": \"mGBDT_Linear\",\n",
    "                                            \"pred\": net_linear.forward(X_test).reshape(-1,),\n",
    "                                            \"y\": y_test\n",
    "                                           })\n",
    "\n",
    "pred_mgbdt_xgboost = pd.DataFrame.from_dict({\"model\": \"mGBDT_XGBoost\", \n",
    "                                             \"pred\": net_xgboost.forward(X_test).reshape(-1,),\n",
    "                                             \"y\": y_test\n",
    "                                            })\n",
    "\n",
    "pred_linear = pd.DataFrame.from_dict({\"model\": \"Linear\", \n",
    "                                      \"pred\": linear_model.predict(X_test).reshape(-1,),\n",
    "                                      \"y\": y_test\n",
    "                                     })\n",
    "\n",
    "\n",
    "pred_xgboost = pd.DataFrame.from_dict({\"model\": \"XGBoost\",\n",
    "                                       \"pred\": xgb_model.predict(dtest),\n",
    "                                       \"y\": y_test\n",
    "                                      })\n",
    "\n",
    "pred_df = pd.concat([pred_mgbdt_linear, pred_mgbdt_xgboost, pred_linear, pred_xgboost])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7e835-b89c-40c5-9903-95406a846768",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9054fa3-bec4-43b3-8e6c-2702a407d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_96020\\1387413843.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  pred_df.groupby(\"model\").apply(lambda x: mean_absolute_error(y_true=x.y, y_pred=x.pred)).reset_index().rename(columns={0: \"MAE\"}).sort_values(by=\"MAE\").reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>2.377509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mGBDT_Linear</td>\n",
       "      <td>3.217939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mGBDT_XGBoost</td>\n",
       "      <td>6.272753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear</td>\n",
       "      <td>72.751922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model        MAE\n",
       "0        XGBoost   2.377509\n",
       "1   mGBDT_Linear   3.217939\n",
       "2  mGBDT_XGBoost   6.272753\n",
       "3         Linear  72.751922"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.groupby(\"model\").apply(lambda x: mean_absolute_error(y_true=x.y, y_pred=x.pred)).reset_index().rename(columns={0: \"MAE\"}).sort_values(by=\"MAE\").reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
